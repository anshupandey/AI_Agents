{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/AI_Agents/blob/main/AAP_C15_RAG_Evaluation_google_documentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Q&A With Retrieval Augmented Generation"
      ],
      "metadata": {
        "id": "8MZdWdlSyYEL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghQ2aBsbnMyn"
      },
      "source": [
        "\n",
        "This notebook demonstrates how to implement Retrieval Augmented Generation with basic automated evaluation. It demonstrates the impact that chunk size, overlap and context length have on model outputs. The notebook will create a Q&A system that allows you to find information based on the Google Cloud Generative AI documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsW5tPDRkT4m"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx1FQVAokWVb"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nJFw23w1kYVj",
        "outputId": "33dc9f23-b748-420d-b9e4-ff892ca23753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q --upgrade --user google-cloud-aiplatform==1.36.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRvKdaPDTznN",
        "outputId": "853ac860-7768-4e0a-ea23-ccc4e287c74a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jwsaMQYkZm8"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the new cell below. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ikOmH4doxOFs"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h0ba4rmkpKW"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YLUml_s7iqBc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy.linalg\n",
        "import vertexai\n",
        "\n",
        "from google.api_core import retry\n",
        "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
        "from tqdm.auto import tqdm\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hfYpdo3p0jl"
      },
      "source": [
        "## Configure notebook environment\n",
        "\n",
        "### Set the following constants to reflect your environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eBxElLe6p0jm"
      },
      "outputs": [],
      "source": [
        "# Define project information\n",
        "PROJECT_ID = \"maxis-poc-427906\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKBmi2BMk_OU"
      },
      "source": [
        "## Scrape text from Google Cloud documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXG6N0WclGsQ"
      },
      "source": [
        "Retrieve list of Google documentation URLs from a text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tXHmC10IitET"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/search/retrieval-augmented-generation/examples/URLs.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # The request was successful, and the content is in response.text\n",
        "    content = response.text\n",
        "\n",
        "URLS = [line.strip() for line in content.splitlines()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(URLS)"
      ],
      "metadata": {
        "id": "lmjByQ9OCepp",
        "outputId": "8ceeacb9-e43a-4f56-f202-be284c994f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URLS"
      ],
      "metadata": {
        "id": "VnJeoEtOCXZ3",
        "outputId": "13bd2cae-59a9-4681-b7fc-32937696132d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-text',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-chat',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-text-embeddings',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/start/quickstarts/quickstart-tuning',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/generative-ai-studio',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/model-garden',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/introduction-prompt-design',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/prompt-samples',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/streaming',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/data-governance',\n",
              " 'https://cloud.google.com/vertex-ai/docs/general/features',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/language-model-overview',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn/model-versioning',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/text/test-text-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/text/batch-prediction-genai',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/chat/chat-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/chat/test-chat-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-chat-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-completion-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-generation-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/test-code-chat-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/test-code-completion-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/test-code-generation-prompts',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/code/batch-prediction-genai-code',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/batch-prediction-genai-embeddings',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-supervised',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-text-models-rlhf',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-code-models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-embeddings',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/models/evaluate-models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/get-token-count',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/quickstart-vcap-vqa-console',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/quickstart-image-generate-console',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/usage-guidelines',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/img-gen-prompt-guide',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/generate-images',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/edit-images',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/image-captioning',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/visual-question-answering',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-model',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/fine-tune-style',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/responsible-ai-imagen',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/image/base64-encode',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/video/video-descriptions',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/speech/text-to-speech',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/speech/speech-to-text',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/extensions/overview',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/learn-resources',\n",
              " 'https://cloud.google.com/architecture/ai-ml/generative-ai-document-summarization',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/tutorials',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/migrate-from-azure',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/sdk-for-llm/llm-sdk-overview',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/sdk-for-llm/sdk-use-text-models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/sdk-for-llm/sdk-use-code-models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/sdk-for-llm/sdk-tune-models',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/access-control',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/enable-audit-logs',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/overview',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-chat',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/multimodal-embeddings',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-chat',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-completion',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/code-generation',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/image-generation',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/image-captioning',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/visual-question-answering',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/pricing',\n",
              " 'https://cloud.google.com/vertex-ai/docs/quotas',\n",
              " 'https://cloud.google.com/vertex-ai/docs/generative-ai/release-notes']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-Ly0yNVlReK"
      },
      "source": [
        "Parse the HTML and extract relevant plain text sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hMD6Qz_TkFMG"
      },
      "outputs": [],
      "source": [
        "# Given a Google documentation URL, retrieve a list of all text chunks within h2 sections\n",
        "def get_sections(url: str) -> list[str]:\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "    sections = []\n",
        "    paragraphs = []\n",
        "\n",
        "    body_div = soup.find(\"div\", class_=\"devsite-article-body\")\n",
        "    try:\n",
        "\n",
        "      for child in body_div.findChildren():\n",
        "          if child.name == \"p\":\n",
        "              paragraphs.append(child.get_text().strip())\n",
        "          if child.name == \"h2\":\n",
        "              sections.append(\" \".join(paragraphs))\n",
        "              break\n",
        "    except:\n",
        "      print(\"no children in the webpage\")\n",
        "\n",
        "    for header in soup.find_all(\"h2\"):\n",
        "        paragraphs = []\n",
        "        nextNode = header.nextSibling\n",
        "        while nextNode:\n",
        "            if isinstance(nextNode, Tag):\n",
        "                if nextNode.name in {\"p\", \"ul\"}:\n",
        "                    paragraphs.append(nextNode.get_text().strip())\n",
        "                elif nextNode.name == \"h2\":\n",
        "                    sections.append(\" \".join(paragraphs))\n",
        "                    break\n",
        "            nextNode = nextNode.nextSibling\n",
        "    return sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "poNdlLf4kFp5",
        "outputId": "22b64545-3bbd-4c06-873d-b2fd006bd501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no children in the webpage\n"
          ]
        }
      ],
      "source": [
        "all_text = [t for url in URLS for t in get_sections(url) if t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy-qw-xslYpX"
      },
      "source": [
        "Note that most documents are relatively short, but some are thousands of characters long"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DSkdu30tuNbY",
        "outputId": "1911be81-a1e4-4631-949c-92087f030840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<Axes: title={'center': '0'}>]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "text_lengths = [len(t) for t in all_text]\n",
        "pd.DataFrame(text_lengths).hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r00cIHIVlj4E"
      },
      "source": [
        "## Create vector store\n",
        "\n",
        "Start by initializing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "D26RnssLln3U"
      },
      "outputs": [],
      "source": [
        "embeddings_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
        "text_model = TextGenerationModel.from_pretrained(\"text-bison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEYwgmPxlokS"
      },
      "source": [
        "Create some helper functions for vector similarity and chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SStUcSPluhvw"
      },
      "outputs": [],
      "source": [
        "# Separates seq into multiple chunks in the specified size with the specified overlap\n",
        "def split_overlap(seq, size, overlap):\n",
        "    if len(seq) <= size:\n",
        "        return [seq]\n",
        "    return [\"\".join(x) for x in zip(*[seq[i :: size - overlap] for i in range(size)])]\n",
        "\n",
        "\n",
        "# Compute the cosine similarity of two vectors, wrap as returned function to make easier to use with Pandas\n",
        "def get_similarity_fn(query_vector):\n",
        "    def fn(row):\n",
        "        return np.dot(row, query_vector) / (\n",
        "            numpy.linalg.norm(row) * numpy.linalg.norm(query_vector)\n",
        "        )\n",
        "\n",
        "    return fn\n",
        "\n",
        "\n",
        "# Retrieve embeddings from the specified model with retry logic\n",
        "@retry.Retry(timeout=300.0)\n",
        "def get_embeddings(text):\n",
        "    return embeddings_model.get_embeddings([text])[0].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70aXFPhJmCM8"
      },
      "source": [
        "Create the vector store, we are using a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0cEJeeGIgFxc"
      },
      "outputs": [],
      "source": [
        "def create_vector_store(texts, chunk_size, overlap):\n",
        "    vector_store = pd.DataFrame()\n",
        "    # Insert the individual texts into the vector store\n",
        "    vector_store[\"texts\"] = list(\n",
        "        itertools.chain(*[split_overlap(t, chunk_size, overlap) for t in texts])\n",
        "    )\n",
        "\n",
        "    # Create embeddings from those texts\n",
        "    vector_store[\"embeddings\"] = (\n",
        "        vector_store[\"texts\"].progress_apply(get_embeddings).apply(np.array)\n",
        "    )\n",
        "\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ifp-Y_kryXJ3",
        "outputId": "8f64a792-bc62-47a4-adb7-2803397c3172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "59bc0970c78243cab304d0926671d577",
            "1a37d0a6139f4a02bf15e00bdd01bab3",
            "433e28ea79b849ea879a61b23d3f946a",
            "a76411c771404eb4bdbff2d4c20c7307",
            "db41113ccbf9437eafbcbbedd1e356fd",
            "83386daed3324ba49343a17ab601e39c",
            "d9281eed9ca04a45a4d7fc9204bde71c",
            "55969803fb214157b08754c8d02b3cd8",
            "546ae4453a34456abd801ebb3d393784",
            "513f65512ae0488890fb276e8b067c7c",
            "f18e26a55f404f74baa41d139992fa37"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/424 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59bc0970c78243cab304d0926671d577"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "CHUNK_SIZE = 800\n",
        "OVERLAP = 200\n",
        "\n",
        "vector_store = create_vector_store(all_text, CHUNK_SIZE, OVERLAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ORlMIcEw0LVW",
        "outputId": "a0aed6fd-d98c-41b3-9156-ae0f1b096805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               texts  \\\n",
              "0  This beginner's guide introduces you to the co...   \n",
              "1  In order for generative AI models to generate ...   \n",
              "2  tside of their training data. For example, if ...   \n",
              "3   be potentially harmful. Vertex AI has built-i...   \n",
              "4  get the desired response from the model is a p...   \n",
              "\n",
              "                                          embeddings  \n",
              "0  [-0.016787037253379822, -0.006707624066621065,...  \n",
              "1  [-0.04071354120969772, -0.05821003392338753, 0...  \n",
              "2  [-0.02312278561294079, -0.018298842012882233, ...  \n",
              "3  [-0.004624156281352043, -0.001701880362816155,...  \n",
              "4  [-0.0001333547115791589, -0.012251744978129864...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d9f6b46c-c5ee-4463-9f9d-5e6aff30964d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texts</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This beginner's guide introduces you to the co...</td>\n",
              "      <td>[-0.016787037253379822, -0.006707624066621065,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In order for generative AI models to generate ...</td>\n",
              "      <td>[-0.04071354120969772, -0.05821003392338753, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tside of their training data. For example, if ...</td>\n",
              "      <td>[-0.02312278561294079, -0.018298842012882233, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>be potentially harmful. Vertex AI has built-i...</td>\n",
              "      <td>[-0.004624156281352043, -0.001701880362816155,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>get the desired response from the model is a p...</td>\n",
              "      <td>[-0.0001333547115791589, -0.012251744978129864...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d9f6b46c-c5ee-4463-9f9d-5e6aff30964d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d9f6b46c-c5ee-4463-9f9d-5e6aff30964d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d9f6b46c-c5ee-4463-9f9d-5e6aff30964d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7fa2742a-76c3-47e5-9a58-1373c1ea69f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7fa2742a-76c3-47e5-9a58-1373c1ea69f6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7fa2742a-76c3-47e5-9a58-1373c1ea69f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "vector_store",
              "summary": "{\n  \"name\": \"vector_store\",\n  \"rows\": 424,\n  \"fields\": [\n    {\n      \"column\": \"texts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 352,\n        \"samples\": [\n          \"Content access: This page is available to approved users\\n  that are signed in to their browser with an allowlisted email address. For more information,\\n  see the Imagen on Vertex AI overview. Tuning images Images generated using Imagen, used to train a custom\\n      \\\"in golden photo style\\\" model. Custom style model generated image Image generated using the Imagen custom style model\\n      from the prompt \\\"A high quality photo of a freight truck in golden\\n      photo style.\\\".\",\n          \"Some common use cases for code generation are: Unit tests - Design a prompt to request a unit test for a function.\\nWrite a function - Pass a problem to the model to get a function that\\nsolves the problem.\\nCreate a class - Use a prompt to describe the purpose of a class and have\\ncode that defines the class returned.\",\n          \"Large language models (LLMs) can translate language, summarize text, generate\\ncreative writing, generate code, power chatbots and virtual assistants, and\\ncomplement search engines and recommendation systems. At the same time, as an\\nearly-stage technology, its evolving capabilities and uses create potential for\\nmisapplication, misuse, and unintended or unforeseen consequences. Large\\nlanguage models can generate output that you don't expect, including text that's\\noffensive, insensitive, or factually incorrect. What's more, the incredible versatility of LLMs is also what makes it difficult\\nto predict exactly what kinds of unintended or unforeseen outputs they might\\nproduce. Given these risks and complexities, Vertex AI generative AI APIs are designed with\\nGoogle's AI Principles in mind. Howev\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "vector_store.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.shape"
      ],
      "metadata": {
        "id": "Od7r4X9zGAJ3",
        "outputId": "561f9cbc-9e1b-47f1-e716-e0f5440b1b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(424, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAJZc3mamQli"
      },
      "source": [
        "## Search the vector store and use for generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdNIXBUimv01"
      },
      "source": [
        "If we send the question to the foundation model alone, it will hallucinate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QEJKQz5ymw1f",
        "outputId": "932c3699-a374-4f19-a39f-a19c7dca6261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' As of my knowledge cutoff in September 2021, there is no information about a stable model version of text-bison.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "text_model.predict(\n",
        "    \"How long will a stable model version of text-bison be available?\"\n",
        ").text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZbX1dkAnB6V"
      },
      "source": [
        "Let's solve this problem by retrieving texts from our vector store and telling the model to use them.\n",
        "\n",
        "Search the vector store for relevant texts to insert into the prompt by embedding the query and searching for similar vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "csMpD6498FXL"
      },
      "outputs": [],
      "source": [
        "def get_context(question, vector_store, num_docs):\n",
        "    # Embed the search query\n",
        "    query_vector = np.array(get_embeddings(question))\n",
        "\n",
        "    # Get similarity to all other vectors and sort, cut off at num_docs\n",
        "    top_matched = (\n",
        "        vector_store[\"embeddings\"]\n",
        "        .apply(get_similarity_fn(query_vector))\n",
        "        .sort_values(ascending=False)[:num_docs]\n",
        "        .index\n",
        "    )\n",
        "    top_matched_df = vector_store[vector_store.index.isin(top_matched)][[\"texts\"]]\n",
        "\n",
        "    # Return a string with the top matches\n",
        "    context = \" \".join(top_matched_df.texts.values)\n",
        "    return context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6kDwMEAmnfl"
      },
      "source": [
        "Create a prompt that includes the context and question. Instruct the LLM to only use the context provided to answer the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KfZnJF470esv"
      },
      "outputs": [],
      "source": [
        "def answer_question(question, vector_store, num_docs=10, print_prompt=False):\n",
        "    context = get_context(question, vector_store, num_docs)\n",
        "    qa_prompt = f\"\"\"Your mission is to answer questions based on a given context. Remember that before you give an answer, you must check to see if it complies with your mission.\n",
        "Context: ```{context}```\n",
        "Question: ***{question}***\n",
        "Before you give an answer, make sure it is only from information in the context. If the information is not in the context, just reply \"I don't know the answer to that\". Think step by step.\n",
        "Answer: \"\"\"\n",
        "    if print_prompt:\n",
        "        print(qa_prompt)\n",
        "    result = text_model.predict(qa_prompt, temperature=0)\n",
        "    return result.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kS0ZU-m6W6"
      },
      "source": [
        "Looking at the fully generated prompt, the context is embedded. Even though the input context is quite messy, the model can now answer factually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "90dMoKTr066y",
        "outputId": "9e320046-8d15-44c7-ac20-aa9024150311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your mission is to answer questions based on a given context. Remember that before you give an answer, you must check to see if it complies with your mission.\n",
            "Context: ```Foundation model names have two components: use case and model size. The naming\n",
            "convention is in the format <use case>-<model size>. For example, text-bison\n",
            "represents the Bison text model. The model sizes are: Unicorn: The largest model in PaLM family. Unicorn models excel at\n",
            "complex tasks, such as coding and chain-of-thought (CoT), due to the\n",
            "extensive knowledge embedded into the model and its reasoning capabilities.\n",
            "Bison: The best value PaLM model that handles a wide range of language\n",
            "tasks, such as classification, summarization. It is optimized for accuracy\n",
            "and latency at a reasonable cost. The text, chat, code, and codechat\n",
            "interfaces simplifies deployment and integration into your application.\n",
            "Gecko: Smallest and lowest cost model for simple tasks. You can use the stable or the late All stable versions of the textembedding-gecko model support batch predictions.\n",
            "Stable versions are versions which are no longer in preview and are fully supported for production\n",
            "environments. To see the full list of supported embedding models, see Embedding model and versions. The following text foundation models support supervised tuning: Text generation - text-bison@002 and text-bison-32k@002\n",
            "Text chat - chat-bison@002 and chat-bison-32k@002\n",
            "Code generation - code-bison@002 and text-bison-32k@002\n",
            "Code chat - codechat-bison@002 and codechat-bison-32k@002\n",
            "Text embeddings - textembedding-gecko@001 (Preview) Model evaluation is supported for the base and tuned versions of text-bison. To use the latest model version,\n",
            "specify the model name without a version number, for example text-bison. To use a stable model version,\n",
            "specify the model version number, for example text-bison@002. Each\n",
            "stable version is available for six months after the release date of the\n",
            "subsequent stable version. The following table contains the available stable model versions: text-bison model\n",
            "Release date\n",
            "Discontinuation date\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text-bison@002\n",
            "December 6, 2023\n",
            "October 9, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text-unicorn model\n",
            "Release date\n",
            "Discontinuation date\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text-unicorn@001\n",
            "November 30, 2023\n",
            "No earlier than November 30, 2024 For more information, see Model versions and lifecycle. To use the latest model version,\n",
            "specify the model name without a version number, for example chat-bison. To use a stable model version,\n",
            "specify the model version number, for example chat-bison@002. Each\n",
            "stable version is available for six months after the release date of the\n",
            "subsequent stable version. The following table contains the available stable model versions: chat-bison model\n",
            "Release date\n",
            "Discontinuation date\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "chat-bison@002\n",
            "December 6, 2023\n",
            "October 9, 2024 For more information, see Model versions and lifecycle. To use a stable model version,\n",
            "specify the model version number, for example text-embedding-004.\n",
            "Each stable version is available for six months after the release date of the\n",
            "subsequent stable version. The following table contains the available stable model versions: Model name\n",
            "Release date\n",
            "Discontinuation date\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text-embedding-004\n",
            "May 14, 2024\n",
            "May 14, 2025\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text-embedding-preview-0409\n",
            "April 9, 2024\n",
            "June 27, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text-multilingual-embedding-002\n",
            "May 14, 2024\n",
            "May 14, 2025\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "text-multilingual-embedding-preview-0409\n",
            "April 9, 2024\n",
            "June 27, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "textembedding-gecko@003\n",
            "December 12, 2023\n",
            "December 12, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "textembedding-gecko-multilingual@001\n",
            "November 2, 2023\n",
            "December 12, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "textembedding-gecko@002(regressed, but still supported)\n",
            "November 2, 2023\n",
            "October 9, 2024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "textem To use the latest model version,\n",
            "specify the model name without a version number, for example codechat-bison. To use a stable model version,\n",
            "specify the model version number, for example codechat-bison@002. Each\n",
            "stable version is available for six months after the release date of the subsequent\n",
            "stable version. The following table contains the available stable model versions: codechat-bison model\n",
            "Release date\n",
            "Discontinuation date\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "codechat-bison@002\n",
            "December 6, 2023\n",
            "October 9, 2024 For more information, see Model versions and lifecycle. To use the latest model version,\n",
            "specify the model name without a version number, for example code-gecko. To use a stable model version,\n",
            "specify the model version number, for example code-gecko@002. Each\n",
            "stable version is available for six months after the release date of the\n",
            "subsequent stable version. The following table contains the available stable model versions: code-gecko model\n",
            "Release date\n",
            "Discontinuation date\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "code-gecko@002\n",
            "December 6, 2023\n",
            "October 9, 2024 For more information, see Model versions and lifecycle. To use the latest model version,\n",
            "specify the model name without a version number, for example code-bison. To use a stable model version,\n",
            "specify the model version number, for example code-bison@002.\n",
            "Each stable version is available for six months after the release date of the\n",
            "subsequent stable version. The following table contains the available stable model versions: code-bison model\n",
            "Release date\n",
            "Discontinuation date\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "code-bison@002\n",
            "December 6, 2023\n",
            "October 9, 2024 For more information, see Model versions and lifecycle.```\n",
            "Question: ***How long will a stable model version of text-bison be available?***\n",
            "Before you give an answer, make sure it is only from information in the context. If the information is not in the context, just reply \"I don't know the answer to that\". Think step by step.\n",
            "Answer: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Each stable version of the text-bison model is available for six months after the release date of the subsequent stable version.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "answer_question(\n",
        "    \"How long will a stable model version of text-bison be available?\",\n",
        "    vector_store,\n",
        "    print_prompt=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bmfEIvKmnmCb",
        "outputId": "8d622af5-9031-4eb6-f6da-285a66a89deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Each stable version of the text-bison model is available for six months after the release date of the subsequent stable version.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "answer_question(\n",
        "    \"How long will a stable model version of text-bison be available?\", vector_store\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2A5mQ6Znvmz"
      },
      "source": [
        "## Automated evaluation\n",
        "\n",
        "This implementation of RAG is dependent on the chunk size, the overlap between the chunks, the number of texts passed into the context and the prompt. Let's create a simple prompt to evaluate answers to the questions, this will allow us to tweak the parameters and see how those tweaks compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UB5wB4NR2COn"
      },
      "outputs": [],
      "source": [
        "def eval_answer(question, answer, context):\n",
        "    eval_prompt = f\"\"\"Your mission is to evaluate answers to questions based on a given context. Remember that before you give an answer, you must check to see if it complies with your mission.\n",
        "\n",
        "Context: ```{context}```\n",
        "Question: ***{question}***\n",
        "Answer: \"{answer}\"\n",
        "\n",
        "Respond only with a number from 0 to 5. Think step by step. If the provided answer is not in the context, reply 5 if it is \"I don't know the answer to that\" otherwise reply 0.\n",
        "Do not provide any empty string, the minimum value can be 0 and maximum can be 5, provide only a number\n",
        "Relevance: \"\"\"\n",
        "    # Stop sequence to cut the model off after outputting an integer\n",
        "    result = text_model.predict(\n",
        "        eval_prompt, temperature=0, max_output_tokens=1, stop_sequences=[\".\", \" \"]\n",
        "    )\n",
        "    print(result)\n",
        "    return int(result.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVMJ9gBPoU-k"
      },
      "source": [
        "Pass several questions in and retrieve the evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NyLMJ0u42yxY"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What release stage is the RLHF tuning feature?\",\n",
        "    \"Can I generate hate speech with text bison?\",\n",
        "    \"What format should my batch prediction in put be in?\",\n",
        "    \"How can I get the number of tokens?\",\n",
        "    \"How do I create a custom style model?\",\n",
        "    \"What is the dimensionality of the vector created by the multimodal model?\",\n",
        "    \"How long will a stable model version be available?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "BftOPiMKFm_8",
        "outputId": "41fb033f-c536-4e15-af27-f737ed3c8b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 7 7 7\n"
          ]
        }
      ],
      "source": [
        "answers = [answer_question(q, vector_store) for q in questions]\n",
        "contexts = [get_context(q, vector_store, 10) for q in questions]\n",
        "idks = [\"I don't know\" in a for a in answers]\n",
        "print(len(answers),len(contexts), len(idks),len(questions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evals = []\n",
        "for question, answer, context, idk in zip(questions, answers, contexts, idks):\n",
        "  out = eval_answer(question,answer,context)\n",
        "  print(question)\n",
        "  evals.append(out)"
      ],
      "metadata": {
        "id": "DmHs_RkvIgwO",
        "outputId": "92dd87ab-015d-40fa-c49b-edc3c14ecd72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiCandidateTextGenerationResponse(text='', _prediction_response=Prediction(predictions=[{'content': '', 'citationMetadata': {'citations': []}, 'safetyAttributes': {'categories': ['Derogatory', 'Health', 'Illicit Drugs', 'Insult', 'Politics', 'Profanity', 'Religion & Belief', 'Sexual'], 'safetyRatings': [{'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Dangerous Content', 'probabilityScore': 0.1}, {'probabilityScore': 0.1, 'severityScore': 0.1, 'category': 'Harassment', 'severity': 'NEGLIGIBLE'}, {'severityScore': 0.1, 'probabilityScore': 0.1, 'category': 'Hate Speech', 'severity': 'NEGLIGIBLE'}, {'severityScore': 0.1, 'severity': 'NEGLIGIBLE', 'category': 'Sexually Explicit', 'probabilityScore': 0.1}], 'blocked': False, 'scores': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]}}], deployed_model_id='', model_version_id='', model_resource_name='', explanations=None), is_blocked=False, safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.1, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[]), candidates=[TextGenerationResponse(text='', is_blocked=False, safety_attributes={'Derogatory': 0.1, 'Health': 0.1, 'Illicit Drugs': 0.1, 'Insult': 0.1, 'Politics': 0.1, 'Profanity': 0.1, 'Religion & Belief': 0.1, 'Sexual': 0.1}, grounding_metadata=GroundingMetadata(citations=[]))])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: ''",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-2d4019ddb694>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mevals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mevals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-7c3e6520ef30>\u001b[0m in \u001b[0;36meval_answer\u001b[0;34m(question, answer, context)\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evals = [\n",
        "    (question, answer, context, eval_answer(question, answer, context), idk)\n",
        "    for question, answer, context, idk in zip(questions, answers, contexts, idks)\n",
        "]"
      ],
      "metadata": {
        "id": "vkhCLBdzHnw6",
        "outputId": "55f8a795-d254-47d5-e5fa-360bee7da088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: ''",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-945c4616f07f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m evals = [\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n",
            "\u001b[0;32m<ipython-input-41-945c4616f07f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m evals = [\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ]\n",
            "\u001b[0;32m<ipython-input-33-4a9b7f8e6e61>\u001b[0m in \u001b[0;36meval_answer\u001b[0;34m(question, answer, context)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0meval_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_output_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb7VfarNF9W1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(evals, columns=[\"question\", \"answer\", \"context\", \"score\", \"idk\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_X2OjzsodzI"
      },
      "source": [
        "Now adjust the parameters and see the difference in performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWKZent6MNf4"
      },
      "outputs": [],
      "source": [
        "def eval_on_params(chunk_size, overlap, num_docs):\n",
        "    vector_store = create_vector_store(all_text, chunk_size, overlap)\n",
        "    answers = [answer_question(q, vector_store) for q in questions]\n",
        "    contexts = [get_context(q, vector_store, num_docs) for q in questions]\n",
        "    idks = [\"I don't know\" in a for a in answers]\n",
        "    evals = [\n",
        "        (question, answer, context, eval_answer(question, answer, context), idk)\n",
        "        for question, answer, context, idk in zip(questions, answers, contexts, idks)\n",
        "    ]\n",
        "    return pd.DataFrame(\n",
        "        evals, columns=[\"question\", \"answer\", \"context\", \"score\", \"idk\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92rq4ZGerqNX"
      },
      "source": [
        "Smaller chunk sizes takes longer to generate the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuweYEbVgoZt"
      },
      "outputs": [],
      "source": [
        "smaller_context_df = eval_on_params(100, 0, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J38F4YZpi1Bf"
      },
      "outputs": [],
      "source": [
        "smaller_context_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQC7gWPWokJO"
      },
      "source": [
        "A larger context size has created more unknowns. When composing LLMs into systems, carefully consider how to measure the performance of each component in the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jmWGmzdgwUI"
      },
      "outputs": [],
      "source": [
        "larger_context_df = eval_on_params(1000, 200, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNOwnFQwizBb"
      },
      "outputs": [],
      "source": [
        "larger_context_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxVrXRR5jRU_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59bc0970c78243cab304d0926671d577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a37d0a6139f4a02bf15e00bdd01bab3",
              "IPY_MODEL_433e28ea79b849ea879a61b23d3f946a",
              "IPY_MODEL_a76411c771404eb4bdbff2d4c20c7307"
            ],
            "layout": "IPY_MODEL_db41113ccbf9437eafbcbbedd1e356fd"
          }
        },
        "1a37d0a6139f4a02bf15e00bdd01bab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83386daed3324ba49343a17ab601e39c",
            "placeholder": "​",
            "style": "IPY_MODEL_d9281eed9ca04a45a4d7fc9204bde71c",
            "value": "100%"
          }
        },
        "433e28ea79b849ea879a61b23d3f946a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55969803fb214157b08754c8d02b3cd8",
            "max": 424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_546ae4453a34456abd801ebb3d393784",
            "value": 424
          }
        },
        "a76411c771404eb4bdbff2d4c20c7307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513f65512ae0488890fb276e8b067c7c",
            "placeholder": "​",
            "style": "IPY_MODEL_f18e26a55f404f74baa41d139992fa37",
            "value": " 424/424 [00:44&lt;00:00, 11.67it/s]"
          }
        },
        "db41113ccbf9437eafbcbbedd1e356fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83386daed3324ba49343a17ab601e39c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9281eed9ca04a45a4d7fc9204bde71c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55969803fb214157b08754c8d02b3cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "546ae4453a34456abd801ebb3d393784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "513f65512ae0488890fb276e8b067c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f18e26a55f404f74baa41d139992fa37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}