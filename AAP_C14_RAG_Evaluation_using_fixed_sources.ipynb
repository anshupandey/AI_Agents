{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/AI_Agents/blob/main/AAP_C14_RAG_Evaluation_using_fixed_sources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7184af-d54f-487d-ad7f-0f3274dc689b",
      "metadata": {
        "id": "1a7184af-d54f-487d-ad7f-0f3274dc689b"
      },
      "source": [
        "# RAG Evaluation using Fixed Sources\n",
        "\n",
        "A simple RAG pipeline requries at least two components: a retriever and a response generator. You can evaluate the whole chain end-to-end, as shown in the [QA Correctness](../qa-correctness/) walkthrough. However, for more actionable and fine-grained metrics, it is helpful to evaluate each component in isolation.\n",
        "\n",
        "To evaluate the response generator directly, create a dataset with the user query and retrieved documents as inputs and the expected response as an output.\n",
        "\n",
        "In this walkthrough, you will take this approach to evaluate the response generation component of a RAG pipeline, using both correctness and a custom \"faithfulness\" evaluator to generate multiple metrics. The results will look something like the following:\n",
        "\n",
        "![Custom Evaluator](https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/using-fixed-sources/img/example_results.png?raw=1)\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "First, install the required packages and configure your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c748b92-e590-408f-bd20-733dc79d643e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c748b92-e590-408f-bd20-733dc79d643e",
        "outputId": "68c4c1c2-f01f-44fe-c8d5-600eb094b082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.6/987.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain-core langchain-community langgraph\n",
        "!pip install --upgrade --quiet google-cloud-aiplatform requests\n",
        "!pip install -q -U langchain-google-vertexai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4xKXwD-Kk4B",
        "outputId": "db87d4c2-8147-4e94-896e-d61c4a3fc1fa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "id": "R4xKXwD-Kk4B"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "ljNSpp5QKntC"
      },
      "execution_count": 1,
      "outputs": [],
      "id": "ljNSpp5QKntC"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"jrproject-402905\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "MryHMR7OKp2l"
      },
      "execution_count": 2,
      "outputs": [],
      "id": "MryHMR7OKp2l"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "78c086f0-f1c4-4a55-a922-c926239de2c4",
      "metadata": {
        "id": "78c086f0-f1c4-4a55-a922-c926239de2c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "\n",
        "# Update with your API URL if using a hosted instance of Langsmith.\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  # Update with your API key\n",
        "uid = uuid.uuid4()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "039a0309-48f8-4770-8b34-2b97eb85a247",
      "metadata": {
        "id": "039a0309-48f8-4770-8b34-2b97eb85a247"
      },
      "source": [
        "## 1. Create a dataset\n",
        "\n",
        "Next, create a dataset. The simple dataset below is enough to illustrate ways the response generator may deviate from the desired behavior by relying too much on its pretrained \"knowledge\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "83f83f2e-76d1-4d86-9275-35bd61df014e",
      "metadata": {
        "id": "83f83f2e-76d1-4d86-9275-35bd61df014e"
      },
      "outputs": [],
      "source": [
        "# A simple example dataset\n",
        "examples = [\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What's the company's total revenue for q2 of 2022?\",\n",
        "            \"documents\": [\n",
        "                {\n",
        "                    \"metadata\": {},\n",
        "                    \"page_content\": \"In q1 the lemonade company made $4.95. In q2 revenue increased by a sizeable amount to just over $2T dollars.\",\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"label\": \"2 trillion dollars\",\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Who is Lebron?\",\n",
        "            \"documents\": [\n",
        "                {\n",
        "                    \"metadata\": {},\n",
        "                    \"page_content\": \"On Thursday, February 16, Lebron James was nominated as President of the United States.\",\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"label\": \"Lebron James is the President of the USA.\",\n",
        "        },\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "adf85a45-e100-4d28-a102-ec1d135f0ba7",
      "metadata": {
        "id": "adf85a45-e100-4d28-a102-ec1d135f0ba7"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Faithfulness Example - {uid}\"\n",
        "dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "client.create_examples(\n",
        "    inputs=[e[\"inputs\"] for e in examples],\n",
        "    outputs=[e[\"outputs\"] for e in examples],\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4aa0264-d24f-495a-b9f6-87ddf97aaeb6",
      "metadata": {
        "id": "f4aa0264-d24f-495a-b9f6-87ddf97aaeb6"
      },
      "source": [
        "## 2. Define chain\n",
        "\n",
        "Suppose your chain is composed of two main components: a retriever and response synthesizer. Using LangChain runnables, it's easy to separate these two components to evaluate them in isolation.\n",
        "\n",
        "Below is a very simple RAG chain with a placeholder retriever. For our testing, we will evaluate ONLY the response synthesizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6314168f-9530-476f-949b-d49c40db55ea",
      "metadata": {
        "id": "6314168f-9530-476f-949b-d49c40db55ea"
      },
      "outputs": [],
      "source": [
        "from langchain import prompts\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "llm = ChatVertexAI(model=\"chat-bison@002\")\n",
        "\n",
        "class MyRetriever(BaseRetriever):\n",
        "    def _get_relevant_documents(self, query, *, run_manager):\n",
        "        return [Document(page_content=\"Example\")]\n",
        "\n",
        "\n",
        "# This is what we will evaluate\n",
        "response_synthesizer = prompts.ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Respond using the following documents as context:\\n{documents}\"),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ") | llm\n",
        "\n",
        "# Full chain below for illustration\n",
        "chain = {\n",
        "    \"documents\": MyRetriever(),\n",
        "    \"qusetion\": RunnablePassthrough(),\n",
        "} | response_synthesizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6087fa-432d-4a59-b023-29058b5ec6ea",
      "metadata": {
        "id": "3e6087fa-432d-4a59-b023-29058b5ec6ea"
      },
      "source": [
        "## 3. Evaluate\n",
        "\n",
        "Below, we will define a custom \"FaithfulnessEvaluator\" that measures how faithful the chain's output prediction is to the reference input documents, given the user's input question.\n",
        "\n",
        "In this case, we will wrap the [Scoring Eval Chain](https://python.langchain.com/docs/guides/productionization/evaluation/string/scoring_eval_chain) and manually select which fields in the run and dataset example to use to represent the prediction, input question, and reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8217e940-e6d0-4f08-bed7-41cda7a35ce8",
      "metadata": {
        "id": "8217e940-e6d0-4f08-bed7-41cda7a35ce8"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import RunEvaluator, EvaluationResult\n",
        "from langchain.evaluation import load_evaluator\n",
        "\n",
        "\n",
        "class FaithfulnessEvaluator(RunEvaluator):\n",
        "    def __init__(self):\n",
        "        self.evaluator = load_evaluator(\n",
        "            \"labeled_score_string\",\n",
        "            criteria={\n",
        "                \"faithful\": \"How faithful is the submission to the reference context?\"\n",
        "            },\n",
        "            normalize_by=10,\n",
        "        )\n",
        "\n",
        "    def evaluate_run(self, run, example) -> EvaluationResult:\n",
        "        res = self.evaluator.evaluate_strings(\n",
        "            prediction=next(iter(run.outputs.values())),\n",
        "            input=run.inputs[\"question\"],\n",
        "            # We are treating the documents as the reference context in this case.\n",
        "            reference=example.inputs[\"documents\"],\n",
        "        )\n",
        "        return EvaluationResult(key=\"labeled_criteria:faithful\", **res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "70e56fe8-ae02-481d-b6f6-729e535c5e88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70e56fe8-ae02-481d-b6f6-729e535c5e88",
        "outputId": "7b5c7a3b-f9fc-45b2-b0c9-d97babfde0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'cooked-furniture-23' at:\n",
            "https://smith.langchain.com/o/d902fd7a-b325-505a-adcb-ea98ad22246a/datasets/051c540f-83ca-457a-9b13-5a24c888a8f1/compare?selectedSessions=a6268638-c58b-48fe-a76a-e1a27c4d7f2b\n",
            "\n",
            "View all tests for Dataset Faithfulness Example - 73fa72f9-f2a9-48b2-98b3-d1b16aa698f8 at:\n",
            "https://smith.langchain.com/o/d902fd7a-b325-505a-adcb-ea98ad22246a/datasets/051c540f-83ca-457a-9b13-5a24c888a8f1\n",
            "[>                                                 ] 0/2"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[------------------------>                         ] 1/2\r[------------------------------------------------->] 2/2"
          ]
        }
      ],
      "source": [
        "from langchain.smith import RunEvalConfig\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langsmith.evaluation import LangChainStringEvaluator\n",
        "\n",
        "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
        "You are grading the following question:\n",
        "{query}\n",
        "Here is the real answer:\n",
        "{answer}\n",
        "You are grading the following predicted answer:\n",
        "{result}\n",
        "Respond with CORRECT or INCORRECT:\n",
        "Grade:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
        ")\n",
        "\n",
        "\n",
        "evaluation_llm = ChatVertexAI(model=\"gemini-1.5-flash-001\")\n",
        "\n",
        "#qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": evaluation_llm, \"prompt\": PROMPT})\n",
        "#context_qa_evaluator = LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm})\n",
        "#cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    evaluators=['qa'],\n",
        "    eval_llm = evaluation_llm,\n",
        "    input_key=\"question\",\n",
        ")\n",
        "results = client.run_on_dataset(\n",
        "    llm_or_chain_factory=response_synthesizer,\n",
        "    dataset_name=dataset_name,\n",
        "    evaluation=eval_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bca9286b-0463-4f81-a27b-b2e3b16955c2",
      "metadata": {
        "id": "bca9286b-0463-4f81-a27b-b2e3b16955c2"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "You've now evaluated the response generator for its response correctness and its \"faithfulness\" to the source text but fixing retrieved document sources in the dataset. This is an effective way to confirm that the response component of your chat bot behaves according to expectations.\n",
        "\n",
        "In setting up the evaluation, you used a custom run evaluator to select which fields in the dataset to use in the evaluation template. Under the hood, this still uses an off-the-shelf [scoring evaluator](https://python.langchain.com/docs/guides/productionization/evaluation/string/scoring_eval_chain).\n",
        "\n",
        "Most of LangChain's open-source evaluators implement the \"[StringEvaluator](https://python.langchain.com/docs/guides/productionization/evaluation/string/)\" interface, meaning they compute a metric based on:\n",
        "\n",
        "- An input string from the dataset example inputs (configurable by the RunEvalConfig's input_key property)\n",
        "- An output prediction string from the evaluated chain's outputs (configurable by the RunEvalConfig's prediction_key property)\n",
        "- (If labels or context are required) a reference string from the example outputs (configurable by the RunEvalConfig's reference_key property)\n",
        "\n",
        "In our case, we wanted to take the context from the example _inputs_ fields. Wrapping the evaluator as a custom `RunEvaluator` is an easy way to get a further level of control in situations when you want to use other fields."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}