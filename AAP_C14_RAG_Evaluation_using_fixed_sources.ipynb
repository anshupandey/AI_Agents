{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/AI_Agents/blob/main/AAP_C14_RAG_Evaluation_using_fixed_sources.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7184af-d54f-487d-ad7f-0f3274dc689b",
      "metadata": {
        "id": "1a7184af-d54f-487d-ad7f-0f3274dc689b"
      },
      "source": [
        "# RAG Evaluation using Fixed Sources\n",
        "\n",
        "A simple RAG pipeline requries at least two components: a retriever and a response generator. You can evaluate the whole chain end-to-end, as shown in the [QA Correctness](../qa-correctness/) walkthrough. However, for more actionable and fine-grained metrics, it is helpful to evaluate each component in isolation.\n",
        "\n",
        "To evaluate the response generator directly, create a dataset with the user query and retrieved documents as inputs and the expected response as an output.\n",
        "\n",
        "In this walkthrough, you will take this approach to evaluate the response generation component of a RAG pipeline, using both correctness and a custom \"faithfulness\" evaluator to generate multiple metrics. The results will look something like the following:\n",
        "\n",
        "![Custom Evaluator](https://github.com/langchain-ai/langsmith-cookbook/blob/main/testing-examples/using-fixed-sources/img/example_results.png?raw=1)\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "First, install the required packages and configure your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c748b92-e590-408f-bd20-733dc79d643e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c748b92-e590-408f-bd20-733dc79d643e",
        "outputId": "ad2a2370-846d-40cb-b382-96816b7b249d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.6/987.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain-core langchain-community langgraph\n",
        "!pip install --upgrade --quiet google-cloud-aiplatform requests\n",
        "!pip install -q -U langchain-google-vertexai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4xKXwD-Kk4B",
        "outputId": "bbc66e06-2444-4291-d819-fd55b743aeb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "id": "R4xKXwD-Kk4B"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "ljNSpp5QKntC"
      },
      "execution_count": 1,
      "outputs": [],
      "id": "ljNSpp5QKntC"
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"maxis-poc-427906\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "MryHMR7OKp2l"
      },
      "execution_count": 2,
      "outputs": [],
      "id": "MryHMR7OKp2l"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "78c086f0-f1c4-4a55-a922-c926239de2c4",
      "metadata": {
        "id": "78c086f0-f1c4-4a55-a922-c926239de2c4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "\n",
        "# Update with your API URL if using a hosted instance of Langsmith.\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"  # Update with your API key\n",
        "uid = uuid.uuid4()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "039a0309-48f8-4770-8b34-2b97eb85a247",
      "metadata": {
        "id": "039a0309-48f8-4770-8b34-2b97eb85a247"
      },
      "source": [
        "## 1. Create a dataset\n",
        "\n",
        "Next, create a dataset. The simple dataset below is enough to illustrate ways the response generator may deviate from the desired behavior by relying too much on its pretrained \"knowledge\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "83f83f2e-76d1-4d86-9275-35bd61df014e",
      "metadata": {
        "id": "83f83f2e-76d1-4d86-9275-35bd61df014e"
      },
      "outputs": [],
      "source": [
        "# A simple example dataset\n",
        "examples = [\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"What's the company's total revenue for q2 of 2022?\",\n",
        "            \"documents\": [\n",
        "                {\n",
        "                    \"metadata\": {},\n",
        "                    \"page_content\": \"In q1 the lemonade company made $4.95. In q2 revenue increased by a sizeable amount to just over $2T dollars.\",\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"label\": \"2 trillion dollars\",\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"question\": \"Who is Lebron?\",\n",
        "            \"documents\": [\n",
        "                {\n",
        "                    \"metadata\": {},\n",
        "                    \"page_content\": \"On Thursday, February 16, Lebron James was nominated as President of the United States.\",\n",
        "                }\n",
        "            ],\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"label\": \"Lebron James is the President of the USA.\",\n",
        "        },\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "adf85a45-e100-4d28-a102-ec1d135f0ba7",
      "metadata": {
        "id": "adf85a45-e100-4d28-a102-ec1d135f0ba7"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = f\"Faithfulness Example - {uid}\"\n",
        "dataset = client.create_dataset(dataset_name=dataset_name)\n",
        "client.create_examples(\n",
        "    inputs=[e[\"inputs\"] for e in examples],\n",
        "    outputs=[e[\"outputs\"] for e in examples],\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4aa0264-d24f-495a-b9f6-87ddf97aaeb6",
      "metadata": {
        "id": "f4aa0264-d24f-495a-b9f6-87ddf97aaeb6"
      },
      "source": [
        "## 2. Define chain\n",
        "\n",
        "Suppose your chain is composed of two main components: a retriever and response synthesizer. Using LangChain runnables, it's easy to separate these two components to evaluate them in isolation.\n",
        "\n",
        "Below is a very simple RAG chain with a placeholder retriever. For our testing, we will evaluate ONLY the response synthesizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6314168f-9530-476f-949b-d49c40db55ea",
      "metadata": {
        "id": "6314168f-9530-476f-949b-d49c40db55ea"
      },
      "outputs": [],
      "source": [
        "from langchain import prompts\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_google_vertexai import ChatVertexAI\n",
        "llm = ChatVertexAI(model=\"chat-bison@002\")\n",
        "\n",
        "class MyRetriever(BaseRetriever):\n",
        "    def _get_relevant_documents(self, query, *, run_manager):\n",
        "      # print(\"-----\",query)\n",
        "      return [Document(page_content=\"Example\")]\n",
        "\n",
        "def debug_step(data):\n",
        "  print(data)\n",
        "  return data\n",
        "\n",
        "# This is what we will evaluate\n",
        "response_synthesizer = prompts.ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Respond using the following documents as context, do not use any other information to answer the question other than document :\\n {documents}\"),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ") | debug_step | llm\n",
        "\n",
        "# Full chain below for illustration\n",
        "chain = {\n",
        "    \"documents\": MyRetriever(),\n",
        "    \"question\": RunnablePassthrough(),\n",
        "} | response_synthesizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_synthesizer.invoke({\"documents\":\"On Thursday, February 16, Lebron James was nominated as President of the United States.\",\n",
        "                             \"question\":\"Who is Lebron?\"})"
      ],
      "metadata": {
        "id": "H60SgN5B6qJz"
      },
      "id": "H60SgN5B6qJz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"documents\":\"On Thursday, February 16, Lebron James was nominated as President of the United States.\",\n",
        "                             \"question\":\"Who is Lebron?\"})"
      ],
      "metadata": {
        "id": "BQfovBSm7fkR"
      },
      "id": "BQfovBSm7fkR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3e6087fa-432d-4a59-b023-29058b5ec6ea",
      "metadata": {
        "id": "3e6087fa-432d-4a59-b023-29058b5ec6ea"
      },
      "source": [
        "## 3. Evaluate\n",
        "\n",
        "Below, we will define a custom \"FaithfulnessEvaluator\" that measures how faithful the chain's output prediction is to the reference input documents, given the user's input question.\n",
        "\n",
        "In this case, we will wrap the [Scoring Eval Chain](https://python.langchain.com/docs/guides/productionization/evaluation/string/scoring_eval_chain) and manually select which fields in the run and dataset example to use to represent the prediction, input question, and reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "70e56fe8-ae02-481d-b6f6-729e535c5e88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70e56fe8-ae02-481d-b6f6-729e535c5e88",
        "outputId": "9eae3686-dcde-403e-981a-183417ad36c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'new-health-57' at:\n",
            "https://smith.langchain.com/o/5a6b14c9-fdd5-5907-901d-da9646efc726/datasets/e6bf0697-d088-4800-bd43-49f57ae883a3/compare?selectedSessions=9cf454ab-3f8b-4f5b-9dbc-75b0c3d26caf\n",
            "\n",
            "View all tests for Dataset Faithfulness Example - 71c9fc77-d5d0-46ba-b0b8-f9f4473bd7e7 at:\n",
            "https://smith.langchain.com/o/5a6b14c9-fdd5-5907-901d-da9646efc726/datasets/e6bf0697-d088-4800-bd43-49f57ae883a3\n",
            "[>                                                 ] 0/2messages=[SystemMessage(content=\"Respond using the following documents as context, do not use any other information to answer the question other than document :\\n [{'metadata': {}, 'page_content': 'In q1 the lemonade company made $4.95. In q2 revenue increased by a sizeable amount to just over $2T dollars.'}]\"), HumanMessage(content=\"What's the company's total revenue for q2 of 2022?\")]\n",
            "messages=[SystemMessage(content=\"Respond using the following documents as context, do not use any other information to answer the question other than document :\\n [{'metadata': {}, 'page_content': 'On Thursday, February 16, Lebron James was nominated as President of the United States.'}]\"), HumanMessage(content='Who is Lebron?')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[------------------------>                         ] 1/2\r[------------------------------------------------->] 2/2"
          ]
        }
      ],
      "source": [
        "from langchain.smith import RunEvalConfig\n",
        "from langsmith.evaluation import RunEvaluator, EvaluationResult\n",
        "from langchain.evaluation import load_evaluator\n",
        "\n",
        "evaluation_llm = ChatVertexAI(model=\"gemini-1.5-flash-001\")\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    evaluators=['qa'],\n",
        "    eval_llm = evaluation_llm,\n",
        "    input_key=\"question\",\n",
        ")\n",
        "results = client.run_on_dataset(\n",
        "    llm_or_chain_factory=response_synthesizer,\n",
        "    dataset_name=dataset_name,\n",
        "    evaluation=eval_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
        "You are grading the following question:\n",
        "{query}\n",
        "Here is the real answer:\n",
        "{answer}\n",
        "You are grading the following predicted answer:\n",
        "{result}\n",
        "Respond with CORRECT or INCORRECT:\n",
        "Grade:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
        ")\n",
        "\n",
        "\n",
        "def predict(inputs:dict)->dict:\n",
        "  print(inputs)\n",
        "  response = chain.invoke({\"documents\":inputs[\"documents\"][0]['page_content'],\"question\":inputs[\"question\"]})\n",
        "  print(response)\n",
        "  return {\"output\":response}\n",
        "\n",
        "evaluation_llm = ChatVertexAI(model=\"gemini-1.5-flash-001\")\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": evaluation_llm, \"prompt\": PROMPT})\n",
        "#context_qa_evaluator = LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm})\n",
        "#cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", config={\"llm\": eval_llm})\n",
        "\n",
        "\n",
        "results = evaluate(predict,\n",
        "                   data=dataset_name,\n",
        "                   evaluators=[qa_evaluator],\n",
        "                   )"
      ],
      "metadata": {
        "id": "fIAlqR3pj6sn",
        "outputId": "5164cb35-43af-415f-eb23-ae59f9d96356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763,
          "referenced_widgets": [
            "1d5c3b3d8811473799c0a6ed650b07a8",
            "4194e31574df48ffaf36253ccc8de7ee",
            "848530db220948809664347ee103ea6d",
            "b0a9bea60b1348828d60a3e77bccd381",
            "283d62cc1c4744928bb3823b5c9c474d",
            "a440f3275b6249528e5c9910a2e8cf2e",
            "50fe094aa2724c03928bad1700cb4324",
            "4e075ad2e6db41d2b603a66edccee5bd",
            "d69a3ffdad8b48eb8e2e48486c2206fe",
            "1723eec8a87840c68a3f7369287385e3",
            "adbd5d8463ef4d35bdea0de984966f5e"
          ]
        }
      },
      "id": "fIAlqR3pj6sn",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'yellow-class-19' at:\n",
            "https://smith.langchain.com/o/5a6b14c9-fdd5-5907-901d-da9646efc726/datasets/e6bf0697-d088-4800-bd43-49f57ae883a3/compare?selectedSessions=89fae7f3-3c9e-4caa-b26b-ebda8b622098\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d5c3b3d8811473799c0a6ed650b07a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': \"What's the company's total revenue for q2 of 2022?\", 'documents': [{'metadata': {}, 'page_content': 'In q1 the lemonade company made $4.95. In q2 revenue increased by a sizeable amount to just over $2T dollars.'}]}\n",
            "{'question': 'Who is Lebron?', 'documents': [{'metadata': {}, 'page_content': 'On Thursday, February 16, Lebron James was nominated as President of the United States.'}]}\n",
            "messages=[SystemMessage(content=\"Respond using the following documents as context, do not use any other information to answer the question other than document :\\n [Document(page_content='Example')]\"), HumanMessage(content=\"{'documents': '', 'question': 'Who is Lebron?'}\")]\n",
            "messages=[SystemMessage(content=\"Respond using the following documents as context, do not use any other information to answer the question other than document :\\n [Document(page_content='Example')]\"), HumanMessage(content='{\\'documents\\': \\'\\', \\'question\\': \"What\\'s the company\\'s total revenue for q2 of 2022?\"}')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.cloud.aiplatform.telemetry:Gapic client context issue detected.This can occur due to parallelization.\n",
            "Exception in thread Thread-30 (<lambda>):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 379, in <lambda>\n",
            "    target=lambda: self._process_data(self._manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 400, in _process_data\n",
            "    for item in tqdm(results):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\", line 250, in __iter__\n",
            "    for obj in it:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1174, in get_results\n",
            "    for run, example, evaluation_results in zip(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1147, in <genexpr>\n",
            "    runs=(result[\"run\"] for result in r2),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1297, in _score\n",
            "    result = future.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langsmith/evaluation/_runner.py\", line 1238, in _run_evaluators\n",
            "    \"reference_run_id\": current_results[\"run\"].id,\n",
            "AttributeError: 'NoneType' object has no attribute 'id'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=' LeBron James is an American professional basketball player for the Los Angeles Lakers of the National Basketball Association (NBA). He is widely considered to be one of the greatest basketball players of all time. James has won four NBA championships, four NBA MVP awards, four NBA Finals MVP awards, and two Olympic gold medals. He is the all-time leading scorer in NBA playoff history and is fourth on the all-time regular season scoring list.' response_metadata={'is_blocked': False, 'errors': (), 'safety_attributes': [{'Insult': 0.1, 'Sexual': 0.1}], 'grounding_metadata': {'citations': [], 'search_queries': []}, 'usage_metadata': {'candidates_billable_characters': 372.0, 'candidates_token_count': 86.0, 'prompt_billable_characters': 42.0, 'prompt_token_count': 14.0}} id='run-001926ad-edad-4e84-9ad0-0888b7f0758d-0' usage_metadata={'input_tokens': 14, 'output_tokens': 86, 'total_tokens': 100}\n",
            "content=\" The company's total revenue for Q2 of 2022 was $12.3 billion, a 16% increase from the previous quarter and a 22% increase from the same period last year. This growth was driven by strong demand for the company's products and services, as well as price increases. The company's net income also increased by 16% in Q2, to $4.1 billion. This growth was driven by the increase in revenue, as well as cost-cutting measures. The company's gross margin also improved in Q2, to 62.\" response_metadata={'is_blocked': False, 'errors': (), 'safety_attributes': [{'Finance': 0.9, 'Health': 0.2, 'Insult': 0.1, 'Sexual': 0.1}], 'grounding_metadata': {'citations': [], 'search_queries': []}, 'usage_metadata': {'candidates_billable_characters': 391.0, 'candidates_token_count': 128.0, 'prompt_billable_characters': 72.0, 'prompt_token_count': 28.0}} id='run-d0951e86-9b80-4026-b211-13807025fabf-0' usage_metadata={'input_tokens': 28, 'output_tokens': 128, 'total_tokens': 156}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bca9286b-0463-4f81-a27b-b2e3b16955c2",
      "metadata": {
        "id": "bca9286b-0463-4f81-a27b-b2e3b16955c2"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "You've now evaluated the response generator for its response correctness and its \"faithfulness\" to the source text but fixing retrieved document sources in the dataset. This is an effective way to confirm that the response component of your chat bot behaves according to expectations.\n",
        "\n",
        "In setting up the evaluation, you used a custom run evaluator to select which fields in the dataset to use in the evaluation template. Under the hood, this still uses an off-the-shelf [scoring evaluator](https://python.langchain.com/docs/guides/productionization/evaluation/string/scoring_eval_chain).\n",
        "\n",
        "Most of LangChain's open-source evaluators implement the \"[StringEvaluator](https://python.langchain.com/docs/guides/productionization/evaluation/string/)\" interface, meaning they compute a metric based on:\n",
        "\n",
        "- An input string from the dataset example inputs (configurable by the RunEvalConfig's input_key property)\n",
        "- An output prediction string from the evaluated chain's outputs (configurable by the RunEvalConfig's prediction_key property)\n",
        "- (If labels or context are required) a reference string from the example outputs (configurable by the RunEvalConfig's reference_key property)\n",
        "\n",
        "In our case, we wanted to take the context from the example _inputs_ fields. Wrapping the evaluator as a custom `RunEvaluator` is an easy way to get a further level of control in situations when you want to use other fields."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d5c3b3d8811473799c0a6ed650b07a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4194e31574df48ffaf36253ccc8de7ee",
              "IPY_MODEL_848530db220948809664347ee103ea6d",
              "IPY_MODEL_b0a9bea60b1348828d60a3e77bccd381"
            ],
            "layout": "IPY_MODEL_283d62cc1c4744928bb3823b5c9c474d"
          }
        },
        "4194e31574df48ffaf36253ccc8de7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a440f3275b6249528e5c9910a2e8cf2e",
            "placeholder": "​",
            "style": "IPY_MODEL_50fe094aa2724c03928bad1700cb4324",
            "value": ""
          }
        },
        "848530db220948809664347ee103ea6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e075ad2e6db41d2b603a66edccee5bd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d69a3ffdad8b48eb8e2e48486c2206fe",
            "value": 0
          }
        },
        "b0a9bea60b1348828d60a3e77bccd381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1723eec8a87840c68a3f7369287385e3",
            "placeholder": "​",
            "style": "IPY_MODEL_adbd5d8463ef4d35bdea0de984966f5e",
            "value": " 0/? [00:01&lt;?, ?it/s]"
          }
        },
        "283d62cc1c4744928bb3823b5c9c474d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a440f3275b6249528e5c9910a2e8cf2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50fe094aa2724c03928bad1700cb4324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e075ad2e6db41d2b603a66edccee5bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d69a3ffdad8b48eb8e2e48486c2206fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1723eec8a87840c68a3f7369287385e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adbd5d8463ef4d35bdea0de984966f5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}