{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/AI_Agents/blob/main/AAP_UC3_code_retrieval_augmented_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Retrieval Augmented Generation (RAG) with Codey APIs"
      ],
      "metadata": {
        "id": "GL8mcBjqy4ke"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNAEdYNFmQcP"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This notebook demonstrates how you augment output from Codey APIs by bringing in external knowledge. We'll show you an example using Code Retrieval Augmented Generation(RAG) pattern using [Google Cloud's Generative AI github repository](https://github.com/GoogleCloudPlatform/generative-ai) as external knowledge.The notebook uses [Vertex AI PaLM API for Code](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview), [Embeddings for Text API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), FAISS vector store and [LangChain ðŸ¦œï¸ðŸ”—](https://python.langchain.com/en/latest/).\n",
        "\n",
        "### Overview\n",
        "\n",
        "Here is overview of what we'll go over.\n",
        "\n",
        "Index Creation:\n",
        "\n",
        "1. Recursively list the files(.ipynb) in github repo\n",
        "2. Extract code and markdown from the files\n",
        "3. Chunk & generate embeddings for each code strings and add initialize the vector store\n",
        "\n",
        "Runtime:\n",
        "\n",
        "4. User enters a prompt or asks a question as a prompt\n",
        "5. Try zero-shot prompt\n",
        "6. Run prompt using RAG Chain & compare results.To generate response we use **code-bison** however can also use **code-gecko** and **codechat-bison**\n",
        "\n",
        "### Cost\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI PaLM APIs offered by Google Cloud\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
        "\n",
        "**Note:** We are using local vector store(FAISS) for this example however recommend managed highly scalable vector store for production usage such as [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/vector-search/overview) or [AlloyDB for PostgreSQL](https://cloud.google.com/alloydb/docs/ai/work-with-embeddings) or [Cloud SQL for PostgreSQL](https://cloud.google.com/sql/docs/postgres/features)  using pgvector extension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XXl6qTJMqxt"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QHaqV20Csqkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5432691a-d938-4eb1-a8b8-d46bbf98883c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script langsmith is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --user -q google-cloud-aiplatform langchain==0.0.332 faiss-cpu==1.7.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VUWOgz6M1rZ"
      },
      "source": [
        "### Restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BIS8EYgkMy8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbfa8d7-e708-48c0-ecd4-5cdf40efc046"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEDmABVkNBr2"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcP9WBENG0e"
      },
      "source": [
        "If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the cell below. This step is not required if you are using Vertex AI Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1S_HgQXQNcbz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmxMr43Nhoo"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L-Tljm5asMBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806a54d5-49ab-41c0-d176-673f2a72e7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.60.0\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import nbformat\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# LangChain\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import Language\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eNGEcBKG0iK-"
      },
      "outputs": [],
      "source": [
        "# Initialize project\n",
        "# Define project information\n",
        "PROJECT_ID = \"jrproject-402905\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Code Generation\n",
        "code_llm = VertexAI(\n",
        "    model_name=\"code-bison@002\",\n",
        "    max_output_tokens=2048,\n",
        "    temperature=0.1,\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o537exyZk9DI"
      },
      "source": [
        "Next we need to create a GitHub personal token to be able to list all files in a repository.\n",
        "\n",
        "- Follow [this link](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) to create GitHub token with repo->public_repo scope and update `GITHUB_TOKEN` variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bt9IVDSqk7y4"
      },
      "outputs": [],
      "source": [
        "# provide GitHub personal access token\n",
        "GITHUB_TOKEN = \"xxxxxxxxxxxxxxxxxx\"  # @param {type:\"string\"}\n",
        "GITHUB_REPO = \"anshuspandey/generative-ai\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqq3GeEbOJbU"
      },
      "source": [
        "# Index Creation\n",
        "\n",
        "We will be using the Google Cloud Generative AI github repository as the data source. First list all Jupyter Notebook files in the repo and store it in a text file.\n",
        "\n",
        "You can skip this step(#1) if you have executed it once and generated the output text file.\n",
        "\n",
        "### 1. Recursively list the files(.ipynb) in the github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eTA1Jt0uOX8y"
      },
      "outputs": [],
      "source": [
        "# Crawls a GitHub repository and returns a list of all ipynb files in the repository\n",
        "def crawl_github_repo(url: str, is_sub_dir: bool, access_token: str = GITHUB_TOKEN):\n",
        "    ignore_list = [\"__init__.py\"]\n",
        "\n",
        "    if not is_sub_dir:\n",
        "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
        "\n",
        "    else:\n",
        "        api_url = url\n",
        "\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    response = requests.get(api_url, headers=headers)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    files = []\n",
        "\n",
        "    contents = response.json()\n",
        "\n",
        "    for item in contents:\n",
        "        if (\n",
        "            item[\"type\"] == \"file\"\n",
        "            and item[\"name\"] not in ignore_list\n",
        "            and (item[\"name\"].endswith(\".py\") or item[\"name\"].endswith(\".ipynb\"))\n",
        "        ):\n",
        "            files.append(item[\"html_url\"])\n",
        "        elif item[\"type\"] == \"dir\" and not item[\"name\"].startswith(\".\"):\n",
        "            sub_files = crawl_github_repo(item[\"url\"], True)\n",
        "            time.sleep(0.1)\n",
        "            files.extend(sub_files)\n",
        "\n",
        "    return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5vaKaxcGO_R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef6dea5-cd1f-40a1-dbb0-b2387e48dedc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "code_files_urls = crawl_github_repo(GITHUB_REPO, False, GITHUB_TOKEN)\n",
        "\n",
        "# Write list to a file so you do not have to download each time\n",
        "with open(\"code_files_urls.txt\", \"w\") as f:\n",
        "    for item in code_files_urls:\n",
        "        f.write(item + \"\\n\")\n",
        "\n",
        "len(code_files_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c5hoNYJ5byMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78e356b-29c9-4679-f6d0-da1706f2d78c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://github.com/GoogleCloudPlatform/generative-ai/blob/main/conversation/data-store-status-checker/data_store_checker.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/embedding-similarity-visualization.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro-textemb-vectorsearch.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro_embeddings_tuning.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro_multimodal_embeddings.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/use-cases/outlier-detection/bq-vector-search-log-outlier-detection.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/vector-search-quickstart.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "code_files_urls[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFNVieLnR8Ie"
      },
      "source": [
        "### 2. Extract code from the Jupyter notebooks.\n",
        "\n",
        "You could also include .py file, shell scripts etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZsM1M4hn4cBu"
      },
      "outputs": [],
      "source": [
        "# Extracts the python code from an ipynb file from github\n",
        "def extract_python_code_from_ipynb(github_url, cell_type=\"code\"):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n",
        "        \"/blob/\", \"/\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    notebook_content = response.text\n",
        "\n",
        "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "    python_code = None\n",
        "\n",
        "    for cell in notebook.cells:\n",
        "        if cell.cell_type == cell_type:\n",
        "            if not python_code:\n",
        "                python_code = cell.source\n",
        "            else:\n",
        "                python_code += \"\\n\" + cell.source\n",
        "\n",
        "    return python_code\n",
        "\n",
        "\n",
        "def extract_python_code_from_py(github_url):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n",
        "        \"/blob/\", \"/\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    python_code = response.text\n",
        "\n",
        "    return python_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WCRp5Xtb48is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfc23e6-0b3c-4742-cdab-ac511f116b06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "with open(\"code_files_urls.txt\") as f:\n",
        "    code_files_urls = f.read().splitlines()\n",
        "len(code_files_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4Y9SMO7H4xgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906e7600-9cb1-448a-8e2c-d82e894b8deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
            "  validate(nb)\n"
          ]
        }
      ],
      "source": [
        "code_strings = []\n",
        "\n",
        "for i in range(0, len(code_files_urls)):\n",
        "    if code_files_urls[i].endswith(\".ipynb\"):\n",
        "        content = extract_python_code_from_ipynb(code_files_urls[i], \"code\")\n",
        "        doc = Document(\n",
        "            page_content=content, metadata={\"url\": code_files_urls[i], \"file_index\": i}\n",
        "        )\n",
        "        code_strings.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1AF3fhBSLOm"
      },
      "source": [
        "### 3. Chunk & generate embeddings for each code strings & initialize the vector store\n",
        "\n",
        "We need to split code into usable chunks that the LLM can use for code generation. Therefore it's crucial to use the right chunking approach and chunk size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rj1cCA2fqx64"
      },
      "outputs": [],
      "source": [
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oae37l-pvzZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f2a42f7-bdf6-4ab9-c728-61a45300550b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1029\n",
            "Waiting\n",
            "............................................................................................................................................................................................................."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'CustomVertexAIEmbeddings'], vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x794e93687250>, search_kwargs={'k': 5})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Chunk code strings\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
        ")\n",
        "\n",
        "\n",
        "texts = text_splitter.split_documents(code_strings)\n",
        "print(len(texts))\n",
        "\n",
        "# Initialize Embedding API\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        "    model_name=\"textembedding-gecko@latest\",\n",
        ")\n",
        "\n",
        "# Create Index from embedded code chunks\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Init your retriever.\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",  # Also test \"similarity\", \"mmr\"\n",
        "    search_kwargs={\"k\": 5},\n",
        ")\n",
        "\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_gn89IyuHIT"
      },
      "source": [
        "# Runtime\n",
        "### 4. User enters a prompt or asks a question as a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1vrvTkO7uFNi"
      },
      "outputs": [],
      "source": [
        "user_question = \"Create a Python function that takes a prompt and predicts using langchain.llms interface with Vertex AI text-bison model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "azbvOUFRvEp5"
      },
      "outputs": [],
      "source": [
        "# Define prompt templates\n",
        "\n",
        "\n",
        "# Zero Shot prompt template\n",
        "prompt_zero_shot = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct & concise code for to the question below.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Output Code :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_prompt_zero_shot = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=prompt_zero_shot,\n",
        ")\n",
        "\n",
        "\n",
        "# RAG template\n",
        "prompt_RAG = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct code for to the question below. Make sure you follow these rules:\n",
        "    1. Use context to understand the APIs and how to use it & apply.\n",
        "    2. Do not add license information to the output code.\n",
        "    3. Do not include Colab code in the output.\n",
        "    4. Ensure all the requirements in the question are met.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Helpful Response :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_RAG_template = PromptTemplate(\n",
        "    template=prompt_RAG, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_llm(\n",
        "    llm=code_llm,\n",
        "    prompt=prompt_RAG_template,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NBaObAQSlIv"
      },
      "source": [
        "### 5. Try zero-shot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1svTVwtBS0zP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2261f834-2367-4087-974e-f0949516a2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def predict_with_langchain_llms(prompt):\n",
            "    \"\"\"Predicts the next token using the langchain.llms interface with Vertex AI text-bison model.\n",
            "\n",
            "    Args:\n",
            "        prompt: The input text to predict the next token for.\n",
            "\n",
            "    Returns:\n",
            "        The predicted next token.\n",
            "    \"\"\"\n",
            "\n",
            "    # Import the necessary libraries.\n",
            "    import requests\n",
            "\n",
            "    # Set the endpoint and API key.\n",
            "    endpoint = \"https://us-central1-aiplatform.googleapis.com/v1/projects/YOUR_PROJECT_ID/locations/us-central1/models/text-bison-001:predict\"\n",
            "    api_key = \"YOUR_API_KEY\"\n",
            "\n",
            "    # Set the request body.\n",
            "    body = {\n",
            "        \"instances\": [\n",
            "            {\n",
            "                \"text\": prompt,\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "\n",
            "    # Make the request.\n",
            "    response = requests.post(endpoint, json=body, headers={\"Authorization\": f\"Bearer {api_key}\"})\n",
            "\n",
            "    # Get the predicted next token.\n",
            "    predicted_token = response.json()[\"predictions\"][0][\"text\"]\n",
            "\n",
            "    # Return the predicted next token.\n",
            "    return predicted_token\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = code_llm.predict(text=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPm8qdxzwPM0"
      },
      "source": [
        "### 6. Run prompt using RAG Chain & compare results\n",
        "To generate response we use code-bison however can also use code-gecko and codechat-bison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZMz3nPMyVoj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a05705-519d-4d17-83de-b16a5092f609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "from langchain_google_vertexai import ChatVertexAI\n",
            "\n",
            "def predict_with_langchain_llms(prompt):\n",
            "  \"\"\"Predicts using langchain.llms interface with Vertex AI text-bison model.\n",
            "\n",
            "  Args:\n",
            "    prompt: The prompt to predict on.\n",
            "\n",
            "  Returns:\n",
            "    The prediction from the model.\n",
            "  \"\"\"\n",
            "\n",
            "  # Initialize the ChatVertexAI client.\n",
            "  chat = ChatVertexAI(model=\"text-bison-001\")\n",
            "\n",
            "  # Predict the response.\n",
            "  response = chat.invoke([HumanMessage(content=prompt)])\n",
            "\n",
            "  # Return the prediction.\n",
            "  return response.content\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "  # Get the user prompt.\n",
            "  prompt = input(\"Enter your prompt: \")\n",
            "\n",
            "  # Predict the response.\n",
            "  prediction = predict_with_langchain_llms(prompt)\n",
            "\n",
            "  # Print the prediction.\n",
            "  print(f\"Prediction: {prediction}\")\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "results = qa_chain({\"query\": user_question})\n",
        "print(results[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF3lVWK1wjxe"
      },
      "source": [
        "### Let's try another prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jel0ON68XiU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d43364c-dd89-4d28-af6d-07b1408b0e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def get_embeddings_langchain(text):\n",
            "    \"\"\"Gets embeddings for a given text using LangChain with Vertex AI textembedding-gecko model.\n",
            "\n",
            "    Args:\n",
            "        text: The text to get embeddings for.\n",
            "\n",
            "    Returns:\n",
            "        A list of embeddings for the text.\n",
            "    \"\"\"\n",
            "\n",
            "    # Import the necessary libraries.\n",
            "    import google.cloud.aiplatform as aiplatform\n",
            "\n",
            "    # Get the model.\n",
            "    model = aiplatform.gapic.ModelServiceClient.get_model(\n",
            "        name=\"projects/YOUR_PROJECT_ID/locations/YOUR_LOCATION/models/YOUR_MODEL_ID\"\n",
            "    )\n",
            "\n",
            "    # Get the model's input and output tensor names.\n",
            "    input_tensor_name = model.input_tensor_names[0]\n",
            "    output_tensor_name = model.output_tensor_names[0]\n",
            "\n",
            "    # Create the input instance.\n",
            "    input_instance = {\"inputs\": text}\n",
            "\n",
            "    # Get the embeddings.\n",
            "    response = aiplatform.gapic.PredictionServiceClient.predict(\n",
            "        name=model.name, instances=[input_instance]\n",
            "    )\n",
            "\n",
            "    # Get the embeddings from the response.\n",
            "    embeddings = response.predictions[0][output_tensor_name]\n",
            "\n",
            "    # Return the embeddings.\n",
            "    return embeddings\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "user_question = \"Create python function that takes text input and returns embeddings using LangChain with Vertex AI textembedding-gecko model\"\n",
        "\n",
        "\n",
        "response = code_llm.predict(text=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G9bIkqE8sO6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0afa6a-ec2b-422a-e9d0-ba93a95951dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "from langchain.llms import VertexAI\n",
            "from langchain.embeddings import VertexAIEmbeddings\n",
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "from langchain.vectorstores import Chroma\n",
            "from langchain.chains import RetrievalQA\n",
            "\n",
            "# Initialize Vertex AI SDK\n",
            "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
            "\n",
            "# Load documents from GCS\n",
            "loader = GCSDirectoryLoader(\n",
            "    project_name=PROJECT_ID, bucket=\"contractunderstandingatticusdataset\"\n",
            ")\n",
            "documents = loader.load()\n",
            "\n",
            "# Split documents into chunks\n",
            "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
            "docs = text_splitter.split_documents(documents)\n",
            "\n",
            "# Create text embeddings using Vertex AI\n",
            "embedding = VertexAIEmbeddings()\n",
            "contracts_vector_db = Chroma.from_documents(docs, embedding)\n",
            "\n",
            "# Create retriever\n",
            "retriever = contracts_vector_db.as_retriever(\n",
            "    search_type=\"similarity\", search_kwargs={\"k\": 2}\n",
            ")\n",
            "\n",
            "# Create LLM\n",
            "llm = VertexAI(\n",
            "    model_name=\"text-bison-32k\",\n",
            "    max_output_tokens=256,\n",
            "    temperature=0.1,\n",
            "    top_p=0.8,\n",
            "    top_k=40,\n",
            "    verbose=True,\n",
            ")\n",
            "\n",
            "# Create RetrievalQA chain\n",
            "qa_chain = RetrievalQA(llm=llm, retriever=retriever)\n",
            "\n",
            "# Get user question\n",
            "user_question = input(\"Enter your question: \")\n",
            "\n",
            "# Get response from chain\n",
            "response = qa_chain({\"query\": user_question})\n",
            "\n",
            "# Print response\n",
            "print(response[\"result\"])\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "results = qa_chain({\"query\": user_question})\n",
        "print(results[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COCtGyTETQy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m107",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}