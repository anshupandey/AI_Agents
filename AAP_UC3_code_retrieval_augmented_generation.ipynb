{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/AI_Agents/blob/main/AAP_UC3_code_retrieval_augmented_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Retrieval Augmented Generation (RAG) with Codey APIs"
      ],
      "metadata": {
        "id": "GL8mcBjqy4ke"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNAEdYNFmQcP"
      },
      "source": [
        "### Objective\n",
        "\n",
        "This notebook demonstrates how you augment output from Codey APIs by bringing in external knowledge. We'll show you an example using Code Retrieval Augmented Generation(RAG) pattern using [Google Cloud's Generative AI github repository](https://github.com/GoogleCloudPlatform/generative-ai) as external knowledge.The notebook uses [Vertex AI PaLM API for Code](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview), [Embeddings for Text API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), FAISS vector store and [LangChain 🦜️🔗](https://python.langchain.com/en/latest/).\n",
        "\n",
        "### Overview\n",
        "\n",
        "Here is overview of what we'll go over.\n",
        "\n",
        "Index Creation:\n",
        "\n",
        "1. Recursively list the files(.ipynb) in github repo\n",
        "2. Extract code and markdown from the files\n",
        "3. Chunk & generate embeddings for each code strings and add initialize the vector store\n",
        "\n",
        "Runtime:\n",
        "\n",
        "4. User enters a prompt or asks a question as a prompt\n",
        "5. Try zero-shot prompt\n",
        "6. Run prompt using RAG Chain & compare results.To generate response we use **code-bison** however can also use **code-gecko** and **codechat-bison**\n",
        "\n",
        "### Cost\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI PaLM APIs offered by Google Cloud\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
        "\n",
        "**Note:** We are using local vector store(FAISS) for this example however recommend managed highly scalable vector store for production usage such as [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/vector-search/overview) or [AlloyDB for PostgreSQL](https://cloud.google.com/alloydb/docs/ai/work-with-embeddings) or [Cloud SQL for PostgreSQL](https://cloud.google.com/sql/docs/postgres/features)  using pgvector extension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XXl6qTJMqxt"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QHaqV20Csqkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb66eeb5-543f-4267-a0b2-56dc5e3200be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script langsmith is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.6/377.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: The script dotenv is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script humanfriendly is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script watchfiles is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script uvicorn is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script email_validator is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script coloredlogs is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts opentelemetry-bootstrap and opentelemetry-instrument are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script onnxruntime_test is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langsmith is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script fastapi is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script langchain-server is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script chroma is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --user -q google-cloud-aiplatform langchain faiss-cpu\n",
        "!pip install --user --quiet langchain langchain-chroma langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VUWOgz6M1rZ"
      },
      "source": [
        "### Restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BIS8EYgkMy8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9159be65-e6d5-4b00-b340-46fdf3654440"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEDmABVkNBr2"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcP9WBENG0e"
      },
      "source": [
        "If you are running this notebook on Google Colab, you will need to authenticate your environment. To do this, run the cell below. This step is not required if you are using Vertex AI Workbench."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1S_HgQXQNcbz"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVmxMr43Nhoo"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L-Tljm5asMBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12bfcf73-ea47-4e4a-d8fd-80a3b2429b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vertex AI SDK version: 1.60.0\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "import nbformat\n",
        "import requests\n",
        "import time\n",
        "\n",
        "\n",
        "from langchain.schema.document import Document\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import Language\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Vertex AI\n",
        "from google.cloud import aiplatform\n",
        "import vertexai\n",
        "\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-vertexai --quiet"
      ],
      "metadata": {
        "id": "MZEDBrMsd00z",
        "outputId": "3c193574-6290-4be8-a5b2-ab72ea027833",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eNGEcBKG0iK-"
      },
      "outputs": [],
      "source": [
        "# Initialize project\n",
        "# Define project information\n",
        "PROJECT_ID = \"jrproject-402905\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "from langchain_google_vertexai import VertexAI\n",
        "# Code Generation\n",
        "code_llm = VertexAI(\n",
        "    model_name=\"code-bison@002\",\n",
        "    max_output_tokens=2048,\n",
        "    temperature=0.1,\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o537exyZk9DI"
      },
      "source": [
        "Next we need to create a GitHub personal token to be able to list all files in a repository.\n",
        "\n",
        "- Follow [this link](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens) to create GitHub token with repo->public_repo scope and update `GITHUB_TOKEN` variable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bt9IVDSqk7y4"
      },
      "outputs": [],
      "source": [
        "# provide GitHub personal access token\n",
        "GITHUB_TOKEN = \"xxxxxxxxxxxxxxxxxxxxxx\"  # @param {type:\"string\"}\n",
        "GITHUB_REPO = \"GoogleCloudPlatform/generative-ai\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqq3GeEbOJbU"
      },
      "source": [
        "# Index Creation\n",
        "\n",
        "We will be using the Google Cloud Generative AI github repository as the data source. First list all Jupyter Notebook files in the repo and store it in a text file.\n",
        "\n",
        "You can skip this step(#1) if you have executed it once and generated the output text file.\n",
        "\n",
        "### 1. Recursively list the files(.ipynb) in the github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eTA1Jt0uOX8y"
      },
      "outputs": [],
      "source": [
        "# Crawls a GitHub repository and returns a list of all ipynb files in the repository\n",
        "def crawl_github_repo(url: str, is_sub_dir: bool, access_token: str = GITHUB_TOKEN):\n",
        "    ignore_list = [\"__init__.py\"]\n",
        "\n",
        "    if not is_sub_dir:\n",
        "        api_url = f\"https://api.github.com/repos/{url}/contents\"\n",
        "\n",
        "    else:\n",
        "        api_url = url\n",
        "\n",
        "    headers = {\n",
        "        \"Accept\": \"application/vnd.github.v3+json\",\n",
        "        \"Authorization\": f\"Bearer {access_token}\",\n",
        "    }\n",
        "\n",
        "    response = requests.get(api_url, headers=headers)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    files = []\n",
        "\n",
        "    contents = response.json()\n",
        "\n",
        "    for item in contents:\n",
        "        if (\n",
        "            item[\"type\"] == \"file\"\n",
        "            and item[\"name\"] not in ignore_list\n",
        "            and (item[\"name\"].endswith(\".py\") or item[\"name\"].endswith(\".ipynb\"))\n",
        "        ):\n",
        "            files.append(item[\"html_url\"])\n",
        "        elif item[\"type\"] == \"dir\" and not item[\"name\"].startswith(\".\"):\n",
        "            sub_files = crawl_github_repo(item[\"url\"], True)\n",
        "            time.sleep(0.1)\n",
        "            files.extend(sub_files)\n",
        "\n",
        "    return files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5vaKaxcGO_R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d06e2d-496f-4d6d-8bd8-3e2063884d2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "code_files_urls = crawl_github_repo(GITHUB_REPO, False, GITHUB_TOKEN)\n",
        "\n",
        "# Write list to a file so you do not have to download each time\n",
        "with open(\"code_files_urls.txt\", \"w\") as f:\n",
        "    for item in code_files_urls:\n",
        "        f.write(item + \"\\n\")\n",
        "\n",
        "len(code_files_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c5hoNYJ5byMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3068cff7-c22f-4b77-d58d-4b83644bc3c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['https://github.com/GoogleCloudPlatform/generative-ai/blob/main/conversation/data-store-status-checker/data_store_checker.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/embedding-similarity-visualization.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro-textemb-vectorsearch.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro_embeddings_tuning.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro_multimodal_embeddings.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/use-cases/outlier-detection/bq-vector-search-log-outlier-detection.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/vector-search-quickstart.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/chat-completions/intro_chat_completions_api.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/context-caching/intro_context_caching.ipynb',\n",
              " 'https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "code_files_urls[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFNVieLnR8Ie"
      },
      "source": [
        "### 2. Extract code from the Jupyter notebooks.\n",
        "\n",
        "You could also include .py file, shell scripts etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZsM1M4hn4cBu"
      },
      "outputs": [],
      "source": [
        "# Extracts the python code from an ipynb file from github\n",
        "def extract_python_code_from_ipynb(github_url, cell_type=\"code\"):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n",
        "        \"/blob/\", \"/\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    notebook_content = response.text\n",
        "\n",
        "    notebook = nbformat.reads(notebook_content, as_version=nbformat.NO_CONVERT)\n",
        "\n",
        "    python_code = None\n",
        "\n",
        "    for cell in notebook.cells:\n",
        "        if cell.cell_type == cell_type:\n",
        "            if not python_code:\n",
        "                python_code = cell.source\n",
        "            else:\n",
        "                python_code += \"\\n\" + cell.source\n",
        "\n",
        "    return python_code\n",
        "\n",
        "\n",
        "def extract_python_code_from_py(github_url):\n",
        "    raw_url = github_url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n",
        "        \"/blob/\", \"/\"\n",
        "    )\n",
        "\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Check for any request errors\n",
        "\n",
        "    python_code = response.text\n",
        "\n",
        "    return python_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WCRp5Xtb48is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2e177f-d6cf-4af4-d4b3-c8d0d414e31d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "with open(\"code_files_urls.txt\") as f:\n",
        "    code_files_urls = f.read().splitlines()\n",
        "len(code_files_urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4Y9SMO7H4xgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c35b23-3907-4205-8815-384d3caa066d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nbformat/__init__.py:96: MissingIDFieldWarning: Cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
            "  validate(nb)\n"
          ]
        }
      ],
      "source": [
        "code_strings = []\n",
        "\n",
        "for i in range(0, len(code_files_urls)):\n",
        "    if code_files_urls[i].endswith(\".ipynb\"):\n",
        "        content = extract_python_code_from_ipynb(code_files_urls[i], \"code\")\n",
        "        doc = Document(\n",
        "            page_content=content, metadata={\"url\": code_files_urls[i], \"file_index\": i}\n",
        "        )\n",
        "        code_strings.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(code_strings[0].page_content)"
      ],
      "metadata": {
        "id": "_yUdZwzcj6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(code_strings)"
      ],
      "metadata": {
        "id": "_c-asQicj98N",
        "outputId": "c16d7b3c-2906-4824-cfaa-6a802b9cf4da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1AF3fhBSLOm"
      },
      "source": [
        "### 3. Chunk & generate embeddings for each code strings & initialize the vector store\n",
        "\n",
        "We need to split code into usable chunks that the LLM can use for code generation. Therefore it's crucial to use the right chunking approach and chunk size."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai.embeddings import VertexAIEmbeddings"
      ],
      "metadata": {
        "id": "vxWbNvj9liZQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Rj1cCA2fqx64"
      },
      "outputs": [],
      "source": [
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk code strings\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=3000, chunk_overlap=400\n",
        ")\n",
        "\n",
        "\n",
        "texts = text_splitter.split_documents(code_strings)\n",
        "print(len(texts))"
      ],
      "metadata": {
        "id": "vvXWoWAXlVqV",
        "outputId": "26433166-e69c-48f8-ce1d-807cae2a10bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oae37l-pvzZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b34277a-8e48-4a80-b7f0-5ad070197099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting\n",
            "............................................................................................................................................."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'CustomVertexAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x78406650f430>, search_kwargs={'k': 5})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Initialize Embedding API\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        "    model_name=\"textembedding-gecko@latest\",\n",
        ")\n",
        "\n",
        "# Create Index from embedded code chunks\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# Init your retriever.\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",  # Also test \"similarity\", \"mmr\"\n",
        "    search_kwargs={\"k\": 5},\n",
        ")\n",
        "\n",
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"implementing langchain agent with gemini-1.5-flash model\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "for doc in docs:\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "id": "YkuVOGG5o7C4",
        "outputId": "bd3249cf-0688-4454-b984-feffd83aa63e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Copyright 2024 Google LLC\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     https://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "! pip3 install --user install ragas==0.1.6 \\\n",
            "datasets==2.18.0 \\\n",
            "langchain==0.1.14 \\\n",
            "langchain-google-vertexai==1.0.5 \\\n",
            "langchain-chroma==0.1.1 \\\n",
            "chromadb==0.5.0 \\\n",
            "pypdf==4.2.0 \\\n",
            "import IPython\n",
            "\n",
            "app = IPython.Application.instance()\n",
            "app.kernel.do_shutdown(True)\n",
            "import pandas as pd\n",
            "import vertexai\n",
            "\n",
            "from langchain_community.document_loaders import PyPDFLoader\n",
            "from langchain_chroma import Chroma\n",
            "from langchain.chains import RetrievalQA\n",
            "from langchain.prompts import PromptTemplate\n",
            "\n",
            "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
            "\n",
            "# Important to make Gemini Work with RAGAS\n",
            "from ragas.llms.base import LangchainLLMWrapper\n",
            "from ragas import evaluate\n",
            "from ragas.metrics import (\n",
            "    answer_relevancy,\n",
            "    context_recall,\n",
            "    context_precision,\n",
            "    answer_similarity,\n",
            ")\n",
            "from ragas.metrics.critique import harmfulness\n",
            "\n",
            "from datasets import Dataset\n",
            "# TODO(developer): Update the below lines\n",
            "PROJECT_ID = \"<your_project>\"\n",
            "LOCATION = \"<your_region>\"\n",
            "\n",
            "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
            "# Load the Gemini Pro model\n",
            "llm = VertexAI(model_name=\"gemini-1.0-pro-002\")\n",
            "# Load Embeddings Models\n",
            "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
            "# source document\n",
            "document_uri = \"https://arxiv.org/pdf/1706.03762\"\n",
            "# use PyPDF loaded to read and chunk the input document\n",
            "loader = PyPDFLoader(document_uri)\n",
            "docs = loader.load_and_split()\n",
            "\n",
            "# Verify if pages are loaded correctly\n",
            "docs[0]\n",
            "# Create an in-memory Vector DB using Chroma\n",
            "vectordb = Chroma.from_documents(docs, embeddings)\n",
            "# Set Vector DB as retriever\n",
            "retriever = vectordb.as_retriever()\n",
            "# Create Q&A template for the Gemini Model\n",
            "template = \"\"\"You task is to answer questions related documents.\n",
            "Use the following context to answer the question at the end.\n",
            "{context}\n",
            "\n",
            "Answers should be crisp.\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\"\"\"\n",
            "\n",
            "# Create a prompt template for the q&a chain\n",
            "PROMPT = PromptTemplate(\n",
            "    template=template,\n",
            "    input_variables=[\"context\", \"question\"],\n",
            ")\n",
            "\n",
            "# Pass prompts to q&a chain\n",
            "chain_type_kwargs = {\"prompt\": PROMPT}\n",
            "vectorstore.get()\n",
            "# Load from disk\n",
            "vectorstore_disk = Chroma(\n",
            "    persist_directory=\"./chroma_db4\",  # Directory of db\n",
            "    embedding_function=gemini_embeddings,  # Embedding model\n",
            ")\n",
            "# Get the Retriever interface for the store to use later.\n",
            "# When an unstructured query is given to a retriever it will return documents.\n",
            "# Read more about retrievers in the following link.\n",
            "# https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
            "#\n",
            "# Since only 1 document is stored in the Chroma vector store, search_kwargs `k`\n",
            "# is set to 1 to decrease the `k` value of chroma's similarity search from 4 to\n",
            "# 1. If you don't pass this value, you will get a warning.\n",
            "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})\n",
            "from langchain_google_vertexai import ChatVertexAI\n",
            "\n",
            "llm = ChatVertexAI(model_name=\"gemini-1.0-pro\", temperature=0.7)\n",
            "# Prompt template to query Gemini\n",
            "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
            "Use the following context to answer the question.\n",
            "If you don't know the answer, just say that you don't know.\n",
            "Use five sentences maximum and keep the answer concise.\\n\n",
            "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
            "\n",
            "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
            "\n",
            "print(llm_prompt)\n",
            "# Combine data from documents to readable string format.\n",
            "class SimpleLangGraphApp:\n",
            "    def __init__(self, project: str, location: str) -> None:\n",
            "        self.project_id = project\n",
            "        self.location = location\n",
            "\n",
            "    # The set_up method is used to define application initialization logic\n",
            "    def set_up(self) -> None:\n",
            "        model = ChatVertexAI(model=\"gemini-1.5-pro\")\n",
            "\n",
            "        builder = MessageGraph()\n",
            "\n",
            "        model_with_tools = model.bind_tools([get_product_details])\n",
            "        builder.add_node(\"tools\", model_with_tools)\n",
            "\n",
            "        tool_node = ToolNode([get_product_details])\n",
            "        builder.add_node(\"get_product_details\", tool_node)\n",
            "        builder.add_edge(\"get_product_details\", END)\n",
            "\n",
            "        builder.set_entry_point(\"tools\")\n",
            "        builder.add_conditional_edges(\"tools\", router)\n",
            "\n",
            "        self.runnable = builder.compile()\n",
            "\n",
            "    # The query method will be used to send inputs to the agent\n",
            "    def query(self, message: str):\n",
            "        \"\"\"Query the application.\n",
            "\n",
            "        Args:\n",
            "            message: The user message.\n",
            "\n",
            "        Returns:\n",
            "            str: The LLM response.\n",
            "        \"\"\"\n",
            "        chat_history = self.runnable.invoke(HumanMessage(message))\n",
            "\n",
            "        return chat_history[-1].content\n",
            "agent = SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION)\n",
            "agent.set_up()\n",
            "agent.query(message=\"Get product details for shoes\")\n",
            "agent.query(message=\"Get product details for coffee\")\n",
            "agent.query(message=\"Get product details for smartphone\")\n",
            "# Ask a question that cannot be answered using the defined tools\n",
            "agent.query(message=\"Tell me about the weather\")\n",
            "remote_agent = reasoning_engines.ReasoningEngine.create(\n",
            "    SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION),\n",
            "    requirements=[\n",
            "        \"google-cloud-aiplatform[langchain,reasoningengine]\",\n",
            "        \"cloudpickle==3.0.0\",\n",
            "        \"pydantic==2.7.4\",\n",
            "        \"langgraph\",\n",
            "        \"httpx\",\n",
            "    ],\n",
            "    display_name=\"Reasoning Engine with LangGraph\",\n",
            "    description=\"This is a sample custom application in Reasoning Engine that uses LangGraph\",\n",
            "    extra_packages=[],\n",
            ")\n",
            "remote_agent.query(message=\"Get product details for shoes\")\n",
            "remote_agent.query(message=\"Get product details for coffee\")\n",
            "remote_agent.query(message=\"Get product details for smartphone\")\n",
            "remote_agent.query(message=\"Tell me about the weather\")\n",
            "remote_agent.delete()\n",
            "# Verify if pages are loaded correctly\n",
            "docs[0]\n",
            "# Create an in-memory Vector DB using Chroma\n",
            "vectordb = Chroma.from_documents(docs, embeddings)\n",
            "# Set Vector DB as retriever\n",
            "retriever = vectordb.as_retriever()\n",
            "# Create Q&A template for the Gemini Model\n",
            "template = \"\"\"Your task is to answer questions related to documents.\n",
            "Use the following context to answer the question at the end.\n",
            "{context}\n",
            "\n",
            "Answers should be crisp.\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\"\"\"\n",
            "\n",
            "# Create a prompt template for the q&a chain\n",
            "PROMPT = PromptTemplate(\n",
            "    template=template,\n",
            "    input_variables=[\"context\", \"question\"],\n",
            ")\n",
            "\n",
            "# Pass prompts to q&a chain\n",
            "chain_type_kwargs = {\"prompt\": PROMPT}\n",
            "\n",
            "# Retriever arguments\n",
            "retriever.search_kwargs = {\"k\": 1}\n",
            "# Setup a RetrievalQA Chain\n",
            "qa = RetrievalQA.from_chain_type(\n",
            "    llm=custom_chat_model_gemini,\n",
            "    chain_type=\"stuff\",\n",
            "    retriever=retriever,\n",
            "    return_source_documents=True,\n",
            "    chain_type_kwargs=chain_type_kwargs,\n",
            ")\n",
            "# Test the chain with a sample question\n",
            "query = \"Who are the authors of paper on Attention is all you need?\"\n",
            "result = qa({\"query\": query})\n",
            "result\n",
            "# Evaluation set with questions and ground_truth\n",
            "questions = [\n",
            "    \"What architecture is proposed in paper titled Attention is all you need?\",\n",
            "    \"Where do primary authors of paper titled Attention is all you need work?\",\n",
            "]\n",
            "ground_truth = [\"Transformers architecture\", \"Google Brain\"]\n",
            "contexts = []\n",
            "answers = []\n",
            "\n",
            "# Generate contexts and answers for each question\n",
            "for query in questions:\n",
            "    result = qa({\"query\": query})\n",
            "    contexts.append(\n",
            "        [document.page_content for document in result.get(\"source_documents\")]\n",
            "    )\n",
            "    answers.append(result.get(\"result\"))\n",
            "# Convert into a dataset and prepare for consumption by DeepEval API\n",
            "dataset = []\n",
            "for q, a, c, g in itertools.zip_longest(questions, answers, contexts, ground_truth):\n",
            "    dataset.append({\"Question\": q, \"Answer\": g, \"Context\": c})\n",
            "\n",
            "# Inspect the dataset\n",
            "dataset\n",
            "# Base LLM for DeepEval\n",
            "class ProductImageAgent:\n",
            "    \"\"\"An agent that wraps around Gemini 1.5 to extract product attributes from\n",
            "    images.\n",
            "\n",
            "    Args:\n",
            "        gemini_model_version (str): The version string of the Gemini 1.5 model.\n",
            "            gemini-1.5-pro or gemini-1.5-flash\n",
            "        temperature (float): The temperature of the model. Defaults to 1.0.\n",
            "        max_output_tokens (int): The maximum number of output tokens. Defaults to\n",
            "            8192.\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "        gemini_model_version: str = \"gemini-1.5-pro\",\n",
            "        temperature: float = 0.0,\n",
            "        max_output_tokens: int = 8192,\n",
            "    ):\n",
            "        config = GenerationConfig(\n",
            "            temperature=temperature, max_output_tokens=max_output_tokens\n",
            "        )\n",
            "\n",
            "        # System instructions, add any common instructions here.\n",
            "        sys_inst = \"\"\"\n",
            "    As an assistant for an online retailer, your task is to recognize\n",
            "    attributes from the provided product image.\n",
            "    If an attribute vocabulary is provided, please only select attribute values\n",
            "    in that vocabulary. You answer should be strictly consistent with what's in\n",
            "    the image. If any attributes do not exist in the image, please\n",
            "    return null for that attribute.\n",
            "    \"\"\"\n",
            "        self.gemini_model = GenerativeModel(\n",
            "            gemini_model_version, generation_config=config, system_instruction=sys_inst\n",
            "        )\n",
            "\n",
            "    def get_detailed_description(self, image_uri: str, debug: bool = False) -> str:\n",
            "        \"\"\"Generates the detailed product description from an image.\n",
            "\n",
            "        Args:\n",
            "            image_uri: The url to the image, can be a local file path,  or a url\n",
            "            from the web or gcs.\n",
            "\n",
            "        Returns:\n",
            "            The generated detailed description from the model response.\n",
            "        \"\"\"\n",
            "        image_part = Part.from_uri(image_uri, mime_type=get_mime_from_uri(image_uri))\n",
            "        prompt = \"\"\"\n",
            "        Please write a complete and detailed product description for the\n",
            "        above product image. The length of the description should be at least\n",
            "        200 words.\n",
            "        \"\"\"\n",
            "        if debug:\n",
            "            print(\"====== Begin Debug Info ======\")\n",
            "            preview = widgets.HTML(value=f'<img src=\"{image_uri}\" width=\"512\">')\n",
            "            display(preview)\n",
            "            print(f\"Prompt:\\n{prompt}\")\n",
            "            print(\"====== End Debug Info ======\")\n",
            "\n",
            "        model_response = self.gemini_model.generate_content([image_part, prompt])\n",
            "        return model_response.text\n",
            "\n",
            "    def get_attributes(\n",
            "        self,\n",
            "        image_uri: str,\n",
            "        product_category: str = None,\n",
            "        vocabulary_json: str = None,\n",
            "        debug: bool = False,\n",
            "    ) -> str:\n",
            "        \"\"\"Generates the product attributes from an image.\n",
            "\n",
            "        Args:\n",
            "            image_uri (str): The uri of the product image to generate attributes.\n",
            "            vocabulary_json (str): A json string list all the attribute names and\n",
            "            their possible vocabulary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_gn89IyuHIT"
      },
      "source": [
        "# Runtime\n",
        "### 4. User enters a prompt or asks a question as a prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1vrvTkO7uFNi"
      },
      "outputs": [],
      "source": [
        "user_question = \"Create a Python function that takes a prompt and predicts using langchain.llms interface with Vertex AI text-bison model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "azbvOUFRvEp5"
      },
      "outputs": [],
      "source": [
        "# Define prompt templates\n",
        "\n",
        "\n",
        "# Zero Shot prompt template\n",
        "prompt_zero_shot = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct & concise code for to the question below.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Output Code :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_prompt_zero_shot = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=prompt_zero_shot,\n",
        ")\n",
        "\n",
        "\n",
        "# RAG template\n",
        "prompt_RAG = \"\"\"\n",
        "    You are a proficient python developer. Respond with the syntactically correct code for to the question below. Make sure you follow these rules:\n",
        "    1. Use context to understand the APIs and how to use it & apply.\n",
        "    2. Do not add license information to the output code.\n",
        "    3. Do not include Colab code in the output.\n",
        "    4. Ensure all the requirements in the question are met.\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Helpful Response :\n",
        "    \"\"\"\n",
        "\n",
        "prompt_RAG_template = PromptTemplate(\n",
        "    template=prompt_RAG, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_llm(\n",
        "    llm=code_llm,\n",
        "    prompt=prompt_RAG_template,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NBaObAQSlIv"
      },
      "source": [
        "### 5. Try zero-shot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1svTVwtBS0zP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877f8a64-b36e-49b8-fa60-aecd6bee3bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def predict_with_langchain_llms(prompt):\n",
            "    \"\"\"Predicts the next token using the langchain.llms interface with Vertex AI text-bison model.\n",
            "\n",
            "    Args:\n",
            "        prompt: The input text prompt.\n",
            "\n",
            "    Returns:\n",
            "        The predicted next token.\n",
            "    \"\"\"\n",
            "\n",
            "    # Import the necessary libraries.\n",
            "    import requests\n",
            "\n",
            "    # Set the endpoint of the Vertex AI text-bison model.\n",
            "    endpoint = \"https://us-central1-aiplatform.googleapis.com/v1/projects/YOUR_PROJECT_ID/locations/us-central1/models/YOUR_MODEL_ID:predict\"\n",
            "\n",
            "    # Set the request body.\n",
            "    body = {\n",
            "        \"instances\": [\n",
            "            {\n",
            "                \"text\": prompt,\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "\n",
            "    # Make the request to the Vertex AI text-bison model.\n",
            "    response = requests.post(endpoint, json=body)\n",
            "\n",
            "    # Get the predicted next token from the response.\n",
            "    predicted_token = response.json()[\"predictions\"][0][\"text\"]\n",
            "\n",
            "    # Return the predicted next token.\n",
            "    return predicted_token\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = code_llm.predict(text=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPm8qdxzwPM0"
      },
      "source": [
        "### 6. Run prompt using RAG Chain & compare results\n",
        "To generate response we use code-bison however can also use code-gecko and codechat-bison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ZMz3nPMyVoj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed6d559-11a9-4a1d-b6e7-009d75f351e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def predict_with_langchain_llms(prompt: str) -> str:\n",
            "    \"\"\"Predicts using langchain.llms interface with Vertex AI text-bison model.\n",
            "\n",
            "    Args:\n",
            "        prompt: The prompt to predict.\n",
            "\n",
            "    Returns:\n",
            "        The predicted response.\n",
            "    \"\"\"\n",
            "\n",
            "    # Initialize the LLM model.\n",
            "    llm = VertexAI(\n",
            "        model_name=\"text-bison\",\n",
            "        max_output_tokens=256,\n",
            "        temperature=0.1,\n",
            "        top_p=0.8,\n",
            "        top_k=40,\n",
            "        verbose=True,\n",
            "    )\n",
            "\n",
            "    # Predict the response.\n",
            "    response = llm.predict(text=prompt, max_output_tokens=2048, temperature=0.1)\n",
            "\n",
            "    return response\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "results = qa_chain({\"query\": user_question})\n",
        "print(results[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF3lVWK1wjxe"
      },
      "source": [
        "### Let's try another prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jel0ON68XiU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f5585b-4296-4f0e-960a-2ce805a18236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "def get_embeddings_langchain(text):\n",
            "    \"\"\"Gets embeddings for a given text using LangChain with Vertex AI textembedding-gecko model.\n",
            "\n",
            "    Args:\n",
            "        text: The text to get embeddings for.\n",
            "\n",
            "    Returns:\n",
            "        A list of embeddings for the text.\n",
            "    \"\"\"\n",
            "\n",
            "    # Import the necessary libraries.\n",
            "    import google.cloud.aiplatform as aiplatform\n",
            "\n",
            "    # Get the model.\n",
            "    model = aiplatform.gapic.ModelServiceClient.get_model(\n",
            "        name=\"projects/YOUR_PROJECT_ID/locations/YOUR_LOCATION/models/YOUR_MODEL_ID\"\n",
            "    )\n",
            "\n",
            "    # Get the model's input and output tensor names.\n",
            "    input_tensor_name = model.input_tensor_names[0]\n",
            "    output_tensor_name = model.output_tensor_names[0]\n",
            "\n",
            "    # Create the input instance.\n",
            "    input_instance = {\"inputs\": text}\n",
            "\n",
            "    # Get the embeddings.\n",
            "    response = aiplatform.gapic.PredictionServiceClient.predict(\n",
            "        name=model.name, instances=[input_instance]\n",
            "    )\n",
            "\n",
            "    # Get the embeddings from the response.\n",
            "    embeddings = response.predictions[0][output_tensor_name]\n",
            "\n",
            "    # Return the embeddings.\n",
            "    return embeddings\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "user_question = \"Create python function that takes text input and returns embeddings using LangChain with Vertex AI textembedding-gecko model\"\n",
        "\n",
        "\n",
        "response = code_llm.predict(text=user_question, max_output_tokens=2048, temperature=0.1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "G9bIkqE8sO6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c045de8-c170-4d43-c7b8-706b85a0223d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import vertexai\n",
            "from vertexai.language_models import TextEmbeddingModel\n",
            "from vertexai.generative_models import GenerativeModel, ChatSession\n",
            "import os\n",
            "import pandas as pd\n",
            "import chromadb\n",
            "! gsutil -m cp -r gs://github-repo/generative-ai/gemini/use-cases/rag/small-to-big-rag/onlineboutique-codefiles .\n",
            "model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL)\n",
            "\n",
            "\n",
            "def get_text_embedding(doc) -> list:\n",
            "    embeddings = model.get_embeddings([doc])\n",
            "    if len(embeddings) > 1:\n",
            "        raise ValueError(\"More than one embedding returned.\")\n",
            "    if len(embeddings) == 0:\n",
            "        raise ValueError(\"No embedding returned.\")\n",
            "    return embeddings[0].values\n",
            "vertexai.init(project=PROJECT_ID, location=REGION)\n",
            "model = GenerativeModel(GENERATIVE_MODEL)\n",
            "chat = model.start_chat()\n",
            "\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "results = qa_chain({\"query\": user_question})\n",
        "print(results[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "COCtGyTETQy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m107",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m107"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}